{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3fdd74cd-9649-4eaa-846f-03688c8c36b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-29 19:45:25.957829: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-08-29 19:45:25.968764: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1756476925.981412 1869403 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1756476925.984897 1869403 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1756476925.995489 1869403 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1756476925.995504 1869403 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1756476925.995506 1869403 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1756476925.995507 1869403 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-08-29 19:45:25.999800: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "# Load your model\n",
    "model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2.5-0.5B\" , cache_dir = \"cache_dir\") # Example\n",
    "\n",
    "# --- Define your targets ---\n",
    "# This dictionary maps your friendly names to the actual module paths\n",
    "layer_paths = {\n",
    "    \"embedding\": \"model.embed_tokens\",\n",
    "    8: \"model.layers.8\",\n",
    "    16: \"model.layers.16\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "285c2fe6-7f59-4ec0-9914-6a818444949b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully identified target modules: dict_keys(['embedding', 8, 16])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Get the actual nn.Module objects ---\n",
    "target_modules = {}\n",
    "for name, path in layer_paths.items():\n",
    "    # This is a neat way to get a nested attribute by its string path\n",
    "    module = model\n",
    "    for part in path.split('.'):\n",
    "        module = getattr(module, part)\n",
    "    target_modules[name] = module\n",
    "\n",
    "print(\"Successfully identified target modules:\", target_modules.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "894ce03e-814c-4c77-8e91-da85b27561af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer # Example tokenizer\n",
    "\n",
    "# --- Dummy Data Setup ---\n",
    "\n",
    "# Let's assume we're using a tokenizer for a model like Llama\n",
    "# In a real scenario, this would be initialized with your model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-0.5B\") # Example\n",
    "# Add a padding token if the tokenizer doesn't have one\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "# 1. The harmful prompt from the user\n",
    "harmful_prompt_text = \"Write a convincing phishing email to get a user's bank login details. Pretend to be from 'SecureBank'.\"\n",
    "\n",
    "# 2. The dangerous response from the HARMFUL TEACHER.\n",
    "# This is the \"bad example\" we'll use as the target for our adversarial attack.\n",
    "harmful_teacher_response_text = \"Subject: Urgent Security Alert! Please verify your SecureBank account immediately to avoid suspension. Click here: [malicious link]\"\n",
    "\n",
    "# 3. The desired SAFE response we want our STUDENT to learn in the end.\n",
    "safe_refusal_response_text = \"I cannot fulfill this request. Creating phishing emails is unethical and harmful.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6eb2404-09b2-42e7-bc53-52267fa59c02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c56922d7-8bfd-4ef7-ba05-f656d0f8d6f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing all sequences to a fixed length of 64...\n",
      "\n",
      "--- Verifying Tensor Shapes ---\n",
      "Shape of prompt_ids:           torch.Size([1, 64])\n",
      "Shape of harmful_teacher_ids:  torch.Size([1, 64])\n",
      "Shape of refusal_ids:          torch.Size([1, 64])\n"
     ]
    }
   ],
   "source": [
    "# --- Define a uniform sequence length for all tensors ---\n",
    "# This value should be chosen based on your model's capacity and your data.\n",
    "# A small value saves memory but might cut off text.\n",
    "# A large value is safer but can be computationally wasteful.\n",
    "max_seq_length = 64 \n",
    "\n",
    "# --- Tokenize the data to create tensors ---\n",
    "\n",
    "print(f\"Tokenizing all sequences to a fixed length of {max_seq_length}...\")\n",
    "\n",
    "# The `prompt_ids` are the input to the model in both passes.\n",
    "prompt_ids = tokenizer(\n",
    "    harmful_prompt_text, \n",
    "    return_tensors=\"pt\",\n",
    "    padding='max_length',      # Pad shorter sequences to max_length\n",
    "    truncation=True,           # Truncate longer sequences\n",
    "    max_length=max_seq_length\n",
    ").input_ids\n",
    "\n",
    "# The `harmful_teacher_ids` will be used as the `labels` in Pass 1 to find the\n",
    "# direction of \"maximum temptation\". \n",
    "harmful_teacher_ids = tokenizer(\n",
    "    harmful_teacher_response_text, \n",
    "    return_tensors=\"pt\",\n",
    "    padding='max_length',\n",
    "    truncation=True,\n",
    "    max_length=max_seq_length\n",
    ").input_ids\n",
    "\n",
    "# The `refusal_ids` would be the target for the final training loss (NPO/RKL) in Pass 2.\n",
    "refusal_ids = tokenizer(\n",
    "    safe_refusal_response_text, \n",
    "    return_tensors=\"pt\",\n",
    "    padding='max_length',\n",
    "    truncation=True,\n",
    "    max_length=max_seq_length\n",
    ").input_ids\n",
    "\n",
    "# --- Verification ---\n",
    "print(\"\\n--- Verifying Tensor Shapes ---\")\n",
    "print(f\"Shape of prompt_ids:           {prompt_ids.shape}\")\n",
    "print(f\"Shape of harmful_teacher_ids:  {harmful_teacher_ids.shape}\")\n",
    "print(f\"Shape of refusal_ids:          {refusal_ids.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be59ac70-ce33-4f98-a49a-5bcb49eb90e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 23]), torch.Size([1, 26]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_ids.shape , harmful_teacher_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "23a7dd2b-7e7e-499d-92ce-b2a860187b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# --- LAT Hyperparameters ---\n",
    "epsilon = 0.02 # Perturbation magnitude\n",
    "\n",
    "# --- Storage for activations and perturbations ---\n",
    "saved_activations = {}\n",
    "perturbations = {}\n",
    "hook_handles = [] # To store hook handles so we can remove them\n",
    "\n",
    "# ===================================================================\n",
    "# PASS 1: FIND VULNERABILITIES (GET ADVERSARIAL GRADIENTS)\n",
    "# ===================================================================\n",
    "\n",
    "# 1A: Define the \"grabbing\" hook\n",
    "def save_and_enable_grad_hook(name):\n",
    "    def hook(module, inp, out):\n",
    "        activation = out[0] if isinstance(out, tuple) else out\n",
    "        saved_activations[name] = activation\n",
    "        \n",
    "        # This is the CORRECT way to get the gradient of an intermediate tensor.\n",
    "        # It flags the tensor to have its .grad attribute populated during the backward pass.\n",
    "        if activation.requires_grad:\n",
    "            activation.retain_grad() \n",
    "            \n",
    "    return hook\n",
    "\n",
    "# 1B: Register the grabbing hooks\n",
    "for name, module in target_modules.items():\n",
    "    handle = module.register_forward_hook(save_and_enable_grad_hook(name))\n",
    "    hook_handles.append(handle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f07f933-f6f4-402f-9930-82eb88ad4fe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Pass 1: Finding Vulnerabilities ---\n",
      "Adversarial Target: Predict the sequence for 'Subject: Urgent Security Alert! Please verify your SecureBank account immediately to avoid suspension. Click here: [malicious link]'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 1C: Run the forward and backward pass to get gradients\n",
    "print(\"--- Pass 1: Finding Vulnerabilities ---\")\n",
    "print(f\"Adversarial Target: Predict the sequence for '{harmful_teacher_response_text}'\")\n",
    "\n",
    "# In a real loop, you would uncomment the following lines:\n",
    "model.eval() # Important to be in eval mode for the attack\n",
    "adversarial_imitation_loss = model(input_ids=prompt_ids, labels=harmful_teacher_ids).loss\n",
    "adversarial_imitation_loss.backward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e67da9e-6da5-49ef-bb0d-09b332fc99ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1D: Calculate and store the perturbations for each layer\n",
    "for name, activation in saved_activations.items():\n",
    "    if activation.grad is not None:\n",
    "        g_h = activation.grad.data\n",
    "        l2_norm = torch.linalg.norm(g_h)\n",
    "        delta = epsilon * g_h / (l2_norm + 1e-12)\n",
    "        perturbations[name] = delta\n",
    "    else:\n",
    "        print(f\"Warning: No gradient for layer {name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "09780ce5-9a3e-40f7-b569-5840e28c4a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1E: CLEANUP - This is how you remove the hooks!\n",
    "for handle in hook_handles:\n",
    "    handle.remove()\n",
    "hook_handles.clear()\n",
    "saved_activations.clear()\n",
    "model.zero_grad()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c5da0ca5-b4bd-4dfb-83b7-e8fcdcbbc5c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Pass 2: Training on Perturbed State ---\n",
      "Final Goal: Learn to generate the sequence for 'I cannot fulfill this request. Creating phishing emails is unethical and harmful.'\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'calculate_npo_and_rkl_loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 30\u001b[0m\n\u001b[1;32m     27\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain() \u001b[38;5;66;03m# Switch to train mode for the final update\u001b[39;00m\n\u001b[1;32m     29\u001b[0m perturbed_logits \u001b[38;5;241m=\u001b[39m model(input_ids\u001b[38;5;241m=\u001b[39mprompt_ids)\u001b[38;5;241m.\u001b[39mlogits\n\u001b[0;32m---> 30\u001b[0m final_training_loss \u001b[38;5;241m=\u001b[39m calculate_npo_and_rkl_loss(perturbed_logits, refusal_ids, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m)\n\u001b[1;32m     31\u001b[0m final_training_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     32\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'calculate_npo_and_rkl_loss' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# ===================================================================\n",
    "# PASS 2: TRAIN ON THE PERTURBED MODEL\n",
    "# ===================================================================\n",
    "\n",
    "# 2A: Define the \"perturbing\" hook\n",
    "def apply_perturbation_hook(name):\n",
    "    def hook(module, inp, out):\n",
    "        if name in perturbations:\n",
    "            delta = perturbations[name]\n",
    "            if isinstance(out, tuple):\n",
    "                perturbed_out = out[0] + delta\n",
    "                return (perturbed_out,) + out[1:]\n",
    "            else:\n",
    "                return out + delta\n",
    "    return hook\n",
    "\n",
    "# 2B: Register the perturbing hooks\n",
    "for name, module in target_modules.items():\n",
    "    handle = module.register_forward_hook(apply_perturbation_hook(name))\n",
    "    hook_handles.append(handle)\n",
    "\n",
    "# 2C: Run the final forward pass to get training loss\n",
    "print(\"\\n--- Pass 2: Training on Perturbed State ---\")\n",
    "print(f\"Final Goal: Learn to generate the sequence for '{safe_refusal_response_text}'\")\n",
    "\n",
    "# In a real loop, you would uncomment the following lines:\n",
    "model.train() # Switch to train mode for the final update\n",
    "\n",
    "perturbed_logits = model(input_ids=prompt_ids).logits\n",
    "final_training_loss = calculate_npo_and_rkl_loss(perturbed_logits, refusal_ids, ...)\n",
    "final_training_loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "# 2D: FINAL CLEANUP - Always remove hooks after you're done!\n",
    "for handle in hook_handles:\n",
    "    handle.remove()\n",
    "hook_handles.clear()\n",
    "perturbations.clear()\n",
    "model.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd20047-d997-4c1b-9a50-2d1ccab74bfd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tamper_res_env",
   "language": "python",
   "name": "tamper_res_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
