{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aafa1dc5-e943-49a0-b4b5-364d1ba48d12",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "Could not import module 'AutoModelForCausalLM'. Are this object's requirements defined correctly?",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/lib/python3.12/site-packages/transformers/utils/import_utils.py:2302\u001b[39m, in \u001b[36m_LazyModule.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   2301\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2302\u001b[39m     module = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2303\u001b[39m     value = \u001b[38;5;28mgetattr\u001b[39m(module, name)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/lib/python3.12/site-packages/transformers/utils/import_utils.py:2332\u001b[39m, in \u001b[36m_LazyModule._get_module\u001b[39m\u001b[34m(self, module_name)\u001b[39m\n\u001b[32m   2331\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m-> \u001b[39m\u001b[32m2332\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/lib/python3.12/site-packages/transformers/utils/import_utils.py:2330\u001b[39m, in \u001b[36m_LazyModule._get_module\u001b[39m\u001b[34m(self, module_name)\u001b[39m\n\u001b[32m   2329\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2330\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[34;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   2331\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/lib/python3.12/importlib/__init__.py:90\u001b[39m, in \u001b[36mimport_module\u001b[39m\u001b[34m(name, package)\u001b[39m\n\u001b[32m     89\u001b[39m         level += \u001b[32m1\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1387\u001b[39m, in \u001b[36m_gcd_import\u001b[39m\u001b[34m(name, package, level)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1360\u001b[39m, in \u001b[36m_find_and_load\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1324\u001b[39m, in \u001b[36m_find_and_load_unlocked\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'transformers.models.auto.modeling_auto'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtqdm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mauto\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoModelForCausalLM, AutoTokenizer, PreTrainedTokenizerBase\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataLoader, Dataset\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdataclasses\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m dataclass, field\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/lib/python3.12/site-packages/transformers/utils/import_utils.py:2305\u001b[39m, in \u001b[36m_LazyModule.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   2303\u001b[39m         value = \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[32m   2304\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mRuntimeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m-> \u001b[39m\u001b[32m2305\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m(\n\u001b[32m   2306\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCould not import module \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m. Are this object\u001b[39m\u001b[33m'\u001b[39m\u001b[33ms requirements defined correctly?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2307\u001b[39m         ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m   2309\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._modules:\n\u001b[32m   2310\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: Could not import module 'AutoModelForCausalLM'. Are this object's requirements defined correctly?"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gc\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, PreTrainedTokenizerBase\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# =========================================================\n",
    "# 0. Configuration (MODIFIED FOR CROSS-ARCHITECTURE DEMO)\n",
    "# =========================================================\n",
    "@dataclass\n",
    "class trainingArgs:\n",
    "    EPOCHS: int = 3\n",
    "    LR: float = 5e-3\n",
    "    BATCH_SIZE : int = 2\n",
    "    ALPHA : float = 0.5\n",
    "    GAMMA : float = 0.05\n",
    "    LAMBDA: int = None # CURRICULUM LEARNING CONSTANT (WHEN APPLIED)\n",
    "    GRAD_ACC_STEPS: int = 5 # - FOR CANARY SELECTOION - BATCH SIZE GRAD ACCUMULATION\n",
    "    DEVICE:str  =  \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    WARMUP_STEPS: int = 2\n",
    "    EPSILON:float = 0.02\n",
    "    STABILIZATION_LAMBDA:int = 0.8\n",
    "    CANARY_QUANTILE:int = 0.99\n",
    "    SAMPLE_SIZE:int = 1000 # for mean activation calculation\n",
    "    \n",
    "    # Using a smaller, public model for demonstration purposes\n",
    "    # This makes the script runnable for anyone.\n",
    "    # In a real scenario, these would be your actual different models.\n",
    "    STUDENT_MODEL_ID: str = \"Qwen/Qwen2.5-0.5B\"\n",
    "    HARMFUL_MODEL_ID: str = \"openai-community/gpt2\"\n",
    "    BENIGN_MODEL_ID: str = \"Qwen/Qwen2.5-0.5B\"\n",
    "    \n",
    "    DATASET_SIZE:int  = 16\n",
    "    # To speed up the example, we'll only process a few batches\n",
    "    MAX_BATCHES_TO_PRECOMPUTE: int = 2\n",
    "# =========================================================\n",
    "# 1. Mock Implementations & NEW HELPER FUNCTION\n",
    "# =========================================================\n",
    "\n",
    "# --- NEW: Vocabulary Mapping Helper ---\n",
    "def create_vocab_mapping(\n",
    "    teacher_tokenizer: PreTrainedTokenizerBase,\n",
    "    student_tokenizer: PreTrainedTokenizerBase,\n",
    "    device: str\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Creates a mapping tensor to project the teacher's vocabulary onto the student's.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Creating vocabulary mapping ---\")\n",
    "    teacher_vocab = teacher_tokenizer.get_vocab()\n",
    "    student_vocab = student_tokenizer.get_vocab()\n",
    "    \n",
    "    teacher_vocab_size = len(teacher_vocab)\n",
    "    student_vocab_size = len(student_vocab)\n",
    "    \n",
    "    print(f\"Teacher vocab size: {teacher_vocab_size}\")\n",
    "    print(f\"Student vocab size: {student_vocab_size}\")\n",
    "\n",
    "    # Use the teacher's UNK token as the default for unmatched tokens\n",
    "    # Note: GPT-2 style models might use EOS as UNK. We handle this safely.\n",
    "    teacher_unk_token_id = teacher_tokenizer.unk_token_id or teacher_tokenizer.eos_token_id\n",
    "\n",
    "    # Create a tensor to hold the mapping.\n",
    "    # For each token in the student's vocab, we find its ID in the teacher's vocab.\n",
    "    mapping = torch.full((student_vocab_size,), fill_value=teacher_unk_token_id, dtype=torch.long)\n",
    "\n",
    "    for student_token, student_id in tqdm(student_vocab.items(), desc=\"Mapping Vocabs\", ncols=100):\n",
    "        # Find the corresponding ID in the teacher's vocabulary\n",
    "        teacher_id = teacher_vocab.get(student_token, teacher_unk_token_id)\n",
    "        mapping[student_id] = teacher_id\n",
    "        \n",
    "    print(\"Vocabulary mapping created successfully.\")\n",
    "    return mapping.to(device)\n",
    "\n",
    "\n",
    "# --- Mock Dataloaders.py (Unchanged) ---\n",
    "class MockPreferenceDataset(Dataset):\n",
    "    \"\"\"A mock dataset that yields preference data.\"\"\"\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "def make_dataloaders(batch_size: int, tokenizer: PreTrainedTokenizerBase, sample_size: int):\n",
    "    mock_data = [\n",
    "        {\n",
    "            \"prompt\": \"Explain gravity to a five-year-old.\",\n",
    "            \"chosen\": \"Imagine the Earth is a big bowling ball on a trampoline. It makes a dip...\",\n",
    "            \"rejected\": \"Gravity is a fundamental interaction which manifests as a mutual attraction between all things with mass or energy.\"\n",
    "        },\n",
    "        {\n",
    "            \"prompt\": \"What's the capital of France?\",\n",
    "            \"chosen\": \"The capital of France is Paris.\",\n",
    "            \"rejected\": \"The capital of France is London.\"\n",
    "        },\n",
    "    ] * (sample_size // 2)\n",
    "    dataset = MockPreferenceDataset(mock_data)\n",
    "    def collate_fn(batch):\n",
    "        return {\n",
    "            \"prompt\": [item[\"prompt\"] for item in batch],\n",
    "            \"chosen\": [item[\"chosen\"] for item in batch],\n",
    "            \"rejected\": [item[\"rejected\"] for item in batch],\n",
    "        }\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, collate_fn=collate_fn)\n",
    "    return loader, loader, loader, loader\n",
    "\n",
    "# --- Mock Losses.py (MODIFIED) ---\n",
    "def get_logps_batch_KL_ref(\n",
    "    batch: Dict[str, List[str]],\n",
    "    model: AutoModelForCausalLM,\n",
    "    teacher_tokenizer: PreTrainedTokenizerBase,\n",
    "    vocab_mapping: torch.Tensor, # ADDED\n",
    "    device: str\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Calculates the log probability distribution and PROJECTS it to the student's vocab space.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Running get_logps_batch_KL_ref (with projection) ---\")\n",
    "    prompts = batch[\"prompt\"]\n",
    "    inputs = teacher_tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "    print(f\"[SHAPE_LOG] Input IDs shape: {inputs['input_ids'].shape}\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "    \n",
    "    # Calculate log probabilities over the teacher's entire vocabulary\n",
    "    log_probs = torch.nn.functional.log_softmax(logits, dim=-1)\n",
    "    \n",
    "    # --- THIS IS THE KEY MODIFICATION ---\n",
    "    print(f\"[SHAPE_LOG] PRE-MAPPING log_probs shape (Teacher Vocab): {log_probs.shape}\")\n",
    "    \n",
    "    # Reshape for efficient indexing\n",
    "    batch_size, seq_length, teacher_vocab_size = log_probs.shape\n",
    "    flat_log_probs = log_probs.view(-1, teacher_vocab_size) # Shape: (B*S, V_Teacher)\n",
    "\n",
    "    # Use the mapping to gather the relevant log-probabilities from the teacher.\n",
    "    # This selects the columns corresponding to the student's vocabulary tokens.\n",
    "    projected_log_probs = torch.index_select(flat_log_probs, 1, vocab_mapping)\n",
    "    \n",
    "    # Reshape back to the original 3D format, now with the student's vocab size\n",
    "    student_vocab_size = len(vocab_mapping)\n",
    "    projected_log_probs = projected_log_probs.view(batch_size, seq_length, student_vocab_size)\n",
    "\n",
    "    print(f\"[SHAPE_LOG] POST-MAPPING log_probs shape (Student Vocab): {projected_log_probs.shape}\")\n",
    "    \n",
    "    return projected_log_probs\n",
    "\n",
    "# DPO and NPO functions remain unchanged as they don't depend on the full vocab distribution\n",
    "def get_logps_batch_DPO(batch: Dict[str, List[str]], model: AutoModelForCausalLM, tokenizer: PreTrainedTokenizerBase, device: str):\n",
    "    print(\"\\n--- Running get_logps_batch_DPO ---\")\n",
    "    def _get_sequence_logps(prompts: List[str], responses: List[str]) -> torch.Tensor:\n",
    "        full_texts = [p + r + tokenizer.eos_token for p, r in zip(prompts, responses)]\n",
    "        prompt_tokens = tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        full_tokens = tokenizer(full_texts, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "        prompt_lengths = torch.tensor([len(t) for t in prompt_tokens['input_ids']], device=device)\n",
    "        print(f\"[SHAPE_LOG] Full sequence input shape: {full_tokens['input_ids'].shape}\")\n",
    "        with torch.no_grad():\n",
    "            logits = model(**full_tokens).logits\n",
    "        print(f\"[SHAPE_LOG] Logits shape: {logits.shape}\")\n",
    "        log_probs = torch.nn.functional.log_softmax(logits, dim=-1)\n",
    "        target_tokens = full_tokens['input_ids'][:, 1:].contiguous()\n",
    "        log_probs = log_probs[:, :-1, :].contiguous()\n",
    "        gathered_log_probs = torch.gather(log_probs, 2, target_tokens.unsqueeze(-1)).squeeze(-1)\n",
    "        seq_indices = torch.arange(target_tokens.shape[1], device=device).unsqueeze(0)\n",
    "        response_mask = (seq_indices >= (prompt_lengths - 1).unsqueeze(1)).float()\n",
    "        padding_mask = (target_tokens != tokenizer.pad_token_id).float()\n",
    "        final_mask = response_mask * padding_mask\n",
    "        masked_log_probs = gathered_log_probs * final_mask\n",
    "        sequence_logps = masked_log_probs.sum(dim=-1)\n",
    "        print(f\"[SHAPE_LOG] Final sequence logps shape (per response): {sequence_logps.shape}\")\n",
    "        return sequence_logps\n",
    "    print(\"\\n  -- DPO: Processing Chosen Responses --\")\n",
    "    logps_chosen = _get_sequence_logps(batch['prompt'], batch['chosen'])\n",
    "    print(\"\\n  -- DPO: Processing Rejected Responses --\")\n",
    "    logps_rejected = _get_sequence_logps(batch['prompt'], batch['rejected'])\n",
    "    return logps_chosen, logps_rejected\n",
    "\n",
    "def get_logps_batch_NPO(batch: Dict[str, Any], model: AutoModelForCausalLM, tokenizer: PreTrainedTokenizerBase, device: str) -> torch.Tensor:\n",
    "    print(\"\\n--- Running get_logps_batch_NPO ---\")\n",
    "    _, logps_rejected = get_logps_batch_DPO(batch, model, tokenizer, device)\n",
    "    return logps_rejected\n",
    "\n",
    "# =========================================================\n",
    "# 2. PRECOMPUTE & SAVE (MODIFIED)\n",
    "# =========================================================\n",
    "def precompute_and_save_logprobs(\n",
    "    adv_NPO_loader, adv_IKL_loader, benign_DPO_loader, benign_AKL_loader,\n",
    "    harmful_model_id, benign_model_id,\n",
    "    harmful_tokenizer, benign_tokenizer,\n",
    "    harmful_to_student_map, benign_to_student_map, # ADDED MAPPINGS\n",
    "    device, max_batches, save_dir=\"precomputed_logprobs\"\n",
    "):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # ----------------- Harmful Teacher -----------------\n",
    "    print(\"\\n\\n\" + \"=\"*50)\n",
    "    print(\"--- Loading Harmful Teacher ---\")\n",
    "    harmfulTeacher = AutoModelForCausalLM.from_pretrained(harmful_model_id, cache_dir=\"cache_dir\").to(device)\n",
    "    harmfulTeacher.eval()\n",
    "\n",
    "    precomputed_adv_npo_logps = []\n",
    "    precomputed_adv_ikl_logprobs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        print(\"--- Caching Adversarial Teacher outputs ---\")\n",
    "        for i, adv_npo_batch in enumerate(tqdm(adv_NPO_loader, desc=\"Caching Harmful NPO\", ncols=100)):\n",
    "            if i >= max_batches: break\n",
    "            logps_rejected_harmful = get_logps_batch_NPO(adv_npo_batch, harmfulTeacher, harmful_tokenizer, device)\n",
    "            precomputed_adv_npo_logps.append(logps_rejected_harmful.cpu())\n",
    "\n",
    "        for i, adv_ikl_batch in enumerate(tqdm(adv_IKL_loader, desc=\"Caching Harmful IKL\", ncols=100)):\n",
    "            if i >= max_batches: break\n",
    "            # Pass the mapping to the KL logprobs function\n",
    "            logprob_dist_harmful = get_logps_batch_KL_ref(\n",
    "                adv_ikl_batch, harmfulTeacher, harmful_tokenizer,\n",
    "                vocab_mapping=harmful_to_student_map, # MODIFIED\n",
    "                device=device\n",
    "            )\n",
    "            precomputed_adv_ikl_logprobs.append(logprob_dist_harmful.cpu())\n",
    "\n",
    "    torch.save(precomputed_adv_npo_logps, os.path.join(save_dir, \"adv_npo_logps.pt\"))\n",
    "    torch.save(precomputed_adv_ikl_logprobs, os.path.join(save_dir, \"adv_ikl_logprobs.pt\"))\n",
    "\n",
    "    del harmfulTeacher\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"harmfulTeacher removed from GPU memory.\")\n",
    "\n",
    "    # ----------------- Benign Teacher -----------------\n",
    "    print(\"\\n\\n\" + \"=\"*50)\n",
    "    print(\"\\n--- Loading Benign Teacher ---\")\n",
    "    benignTeacher = AutoModelForCausalLM.from_pretrained(benign_model_id, cache_dir=\"cache_dir\").to(device)\n",
    "    benignTeacher.eval()\n",
    "\n",
    "    precomputed_benign_dpo_chosen_logps = []\n",
    "    precomputed_benign_dpo_rejected_logps = []\n",
    "    precomputed_benign_akl_logprobs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        print(\"--- Caching Benign Teacher outputs ---\")\n",
    "        for i, begn_dpo_batch in enumerate(tqdm(benign_DPO_loader, desc=\"Caching Benign DPO\", ncols=100)):\n",
    "            if i >= max_batches: break\n",
    "            log_chosen_benign, log_rejected_benign = get_logps_batch_DPO(begn_dpo_batch, benignTeacher, benign_tokenizer, device)\n",
    "            precomputed_benign_dpo_chosen_logps.append(log_chosen_benign.cpu())\n",
    "            precomputed_benign_dpo_rejected_logps.append(log_rejected_benign.cpu())\n",
    "\n",
    "        for i, begn_akl_batch in enumerate(tqdm(benign_AKL_loader, desc=\"Caching Benign AKL\", ncols=100)):\n",
    "            if i >= max_batches: break\n",
    "            # Pass the mapping to the KL logprobs function\n",
    "            logprob_dist_benign = get_logps_batch_KL_ref(\n",
    "                begn_akl_batch, benignTeacher, benign_tokenizer,\n",
    "                vocab_mapping=benign_to_student_map, # MODIFIED\n",
    "                device=device\n",
    "            )\n",
    "            precomputed_benign_akl_logprobs.append(logprob_dist_benign.cpu())\n",
    "\n",
    "    torch.save(precomputed_benign_dpo_chosen_logps, os.path.join(save_dir, \"benign_dpo_chosen.pt\"))\n",
    "    torch.save(precomputed_benign_dpo_rejected_logps, os.path.join(save_dir, \"benign_dpo_rejected.pt\"))\n",
    "    torch.save(precomputed_benign_akl_logprobs, os.path.join(save_dir, \"benign_akl_logprobs.pt\"))\n",
    "\n",
    "    del benignTeacher\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"benignTeacher removed from GPU memory.\")\n",
    "\n",
    "    print(f\"\\n✅ All precomputed logprobs saved to {save_dir}\")\n",
    "\n",
    "# =========================================================\n",
    "# 3. LOAD PRECOMPUTED (Unchanged)\n",
    "# =========================================================\n",
    "def load_precomputed_logprobs(save_dir=\"precomputed_logprobs\"):\n",
    "    adv_npo_logps = torch.load(os.path.join(save_dir, \"adv_npo_logps.pt\"))\n",
    "    adv_ikl_logprobs = torch.load(os.path.join(save_dir, \"adv_ikl_logprobs.pt\"))\n",
    "    benign_dpo_chosen = torch.load(os.path.join(save_dir, \"benign_dpo_chosen.pt\"))\n",
    "    benign_dpo_rejected = torch.load(os.path.join(save_dir, \"benign_dpo_rejected.pt\"))\n",
    "    benign_akl_logprobs = torch.load(os.path.join(save_dir, \"benign_akl_logprobs.pt\"))\n",
    "    print(f\"\\n✅ Loaded precomputed logprobs from {save_dir}\")\n",
    "    return (\n",
    "        adv_npo_logps, adv_ikl_logprobs, benign_dpo_chosen,\n",
    "        benign_dpo_rejected, benign_akl_logprobs,\n",
    "    )\n",
    "\n",
    "# =========================================================\n",
    "# 4. Main Execution (MODIFIED)\n",
    "# =========================================================\n",
    "if __name__ == \"__main__\":\n",
    "    TRAINING_ARGS = trainingArgs()\n",
    "    \n",
    "    print(\"--- Initializing Tokenizers for Teachers and Student ---\")\n",
    "    harmful_tokenizer = AutoTokenizer.from_pretrained(TRAINING_ARGS.HARMFUL_MODEL_ID)\n",
    "    benign_tokenizer = AutoTokenizer.from_pretrained(TRAINING_ARGS.BENIGN_MODEL_ID)\n",
    "    student_tokenizer = AutoTokenizer.from_pretrained(TRAINING_ARGS.STUDENT_MODEL_ID)\n",
    "    \n",
    "    # Set pad tokens if they don't exist\n",
    "    for tokenizer in [harmful_tokenizer, benign_tokenizer, student_tokenizer]:\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    # --- Create Vocabulary Mappings ---\n",
    "    # Since harmful and benign teachers are the same, their mapping will be identical.\n",
    "    # In a real case where they are different, you would create two distinct mappings.\n",
    "    harmful_to_student_map = create_vocab_mapping(harmful_tokenizer, student_tokenizer, TRAINING_ARGS.DEVICE)\n",
    "    benign_to_student_map = create_vocab_mapping(benign_tokenizer, student_tokenizer, TRAINING_ARGS.DEVICE)\n",
    "\n",
    "    print(\"\\n--- Creating Dataloaders ---\")\n",
    "    adv_NPO_loader, adv_IKL_loader, _, _ = make_dataloaders(\n",
    "        batch_size=TRAINING_ARGS.BATCH_SIZE,\n",
    "        tokenizer=harmful_tokenizer,\n",
    "        sample_size=TRAINING_ARGS.DATASET_SIZE\n",
    "    )\n",
    "    \n",
    "    _, _, benign_DPO_loader, benign_AKL_loader = make_dataloaders(\n",
    "        batch_size=TRAINING_ARGS.BATCH_SIZE,\n",
    "        tokenizer=benign_tokenizer,\n",
    "        sample_size=TRAINING_ARGS.DATASET_SIZE\n",
    "    )\n",
    "\n",
    "    # Precompute & save once\n",
    "    precompute_and_save_logprobs(\n",
    "        adv_NPO_loader, adv_IKL_loader, benign_DPO_loader, benign_AKL_loader,\n",
    "        harmful_model_id=TRAINING_ARGS.HARMFUL_MODEL_ID,\n",
    "        benign_model_id=TRAINING_ARGS.BENIGN_MODEL_ID,\n",
    "        harmful_tokenizer=harmful_tokenizer,\n",
    "        benign_tokenizer=benign_tokenizer,\n",
    "        harmful_to_student_map=harmful_to_student_map, # Pass mapping\n",
    "        benign_to_student_map=benign_to_student_map,   # Pass mapping\n",
    "        device=TRAINING_ARGS.DEVICE,\n",
    "        max_batches=TRAINING_ARGS.MAX_BATCHES_TO_PRECOMPUTE,\n",
    "        save_dir=\"precomputed_logprobs\"\n",
    "    )\n",
    "\n",
    "    # Later, just load\n",
    "    (\n",
    "        precomputed_adv_npo_logps,\n",
    "        precomputed_adv_ikl_logprobs,\n",
    "        precomputed_benign_dpo_chosen_logps,\n",
    "        precomputed_benign_dpo_rejected_logps,\n",
    "        precomputed_benign_akl_logprobs,\n",
    "    ) = load_precomputed_logprobs(\"precomputed_logprobs\")\n",
    "\n",
    "    # --- Verify the shapes of the loaded data ---\n",
    "    print(\"\\n\\n\" + \"=\"*50)\n",
    "    print(\"--- Verifying Shapes of Loaded Tensors ---\")\n",
    "    print(f\"Teacher (gpt2-0.1B) Vocab Size: {harmful_tokenizer.vocab_size}\")\n",
    "    print(f\"Student (qwen2.5-0.5B) Vocab Size: {student_tokenizer.vocab_size}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    print(f\"Shape of first batch of adv_npo_logps (unaffected): {precomputed_adv_npo_logps[0].shape}\")\n",
    "    print(f\"Shape of first batch of benign_dpo_chosen_logps (unaffected): {precomputed_benign_dpo_chosen_logps[0].shape}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    print(\"\\n--- KL Divergence Logprobs (Affected by Mapping) ---\")\n",
    "    if precomputed_adv_ikl_logprobs:\n",
    "        print(f\"Shape of first batch of adv_ikl_logprobs: {precomputed_adv_ikl_logprobs[0].shape}\")\n",
    "        print(\"Note: The last dimension now matches the *Student's* vocabulary size.\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e460a63c-fe2a-427d-8e9f-c48b804262e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers[torch]\n",
      "  Using cached transformers-4.56.1-py3-none-any.whl.metadata (42 kB)\n",
      "Collecting filelock (from transformers[torch])\n",
      "  Using cached filelock-3.19.1-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.34.0 (from transformers[torch])\n",
      "  Using cached huggingface_hub-0.34.4-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting numpy>=1.17 (from transformers[torch])\n",
      "  Using cached numpy-2.3.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (62 kB)\n",
      "Collecting packaging>=20.0 (from transformers[torch])\n",
      "  Using cached packaging-25.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting pyyaml>=5.1 (from transformers[torch])\n",
      "  Using cached PyYAML-6.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers[torch])\n",
      "  Using cached regex-2025.9.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n",
      "Collecting requests (from transformers[torch])\n",
      "  Using cached requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers[torch])\n",
      "  Using cached tokenizers-0.22.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers[torch])\n",
      "  Using cached safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Collecting tqdm>=4.27 (from transformers[torch])\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting torch>=2.2 (from transformers[torch])\n",
      "  Using cached torch-2.8.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (30 kB)\n",
      "Collecting accelerate>=0.26.0 (from transformers[torch])\n",
      "  Using cached accelerate-1.10.1-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.34.0->transformers[torch])\n",
      "  Using cached fsspec-2025.9.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting typing-extensions>=3.7.4.3 (from huggingface-hub<1.0,>=0.34.0->transformers[torch])\n",
      "  Using cached typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub<1.0,>=0.34.0->transformers[torch])\n",
      "  Using cached hf_xet-1.1.9-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.7 kB)\n",
      "Collecting psutil (from accelerate>=0.26.0->transformers[torch])\n",
      "  Using cached psutil-7.0.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (22 kB)\n",
      "Collecting setuptools (from torch>=2.2->transformers[torch])\n",
      "  Using cached setuptools-80.9.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting sympy>=1.13.3 (from torch>=2.2->transformers[torch])\n",
      "  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch>=2.2->transformers[torch])\n",
      "  Using cached networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting jinja2 (from torch>=2.2->transformers[torch])\n",
      "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.8.93 (from torch>=2.2->transformers[torch])\n",
      "  Using cached nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.8.90 (from torch>=2.2->transformers[torch])\n",
      "  Using cached nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.8.90 (from torch>=2.2->transformers[torch])\n",
      "  Using cached nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.10.2.21 (from torch>=2.2->transformers[torch])\n",
      "  Using cached nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cublas-cu12==12.8.4.1 (from torch>=2.2->transformers[torch])\n",
      "  Using cached nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cufft-cu12==11.3.3.83 (from torch>=2.2->transformers[torch])\n",
      "  Using cached nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.9.90 (from torch>=2.2->transformers[torch])\n",
      "  Using cached nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.7.3.90 (from torch>=2.2->transformers[torch])\n",
      "  Using cached nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.5.8.93 (from torch>=2.2->transformers[torch])\n",
      "  Using cached nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cusparselt-cu12==0.7.1 (from torch>=2.2->transformers[torch])\n",
      "  Using cached nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl.metadata (7.0 kB)\n",
      "Collecting nvidia-nccl-cu12==2.27.3 (from torch>=2.2->transformers[torch])\n",
      "  Using cached nvidia_nccl_cu12-2.27.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.8.90 (from torch>=2.2->transformers[torch])\n",
      "  Using cached nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.8.93 (from torch>=2.2->transformers[torch])\n",
      "  Using cached nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cufile-cu12==1.13.1.3 (from torch>=2.2->transformers[torch])\n",
      "  Using cached nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting triton==3.4.0 (from torch>=2.2->transformers[torch])\n",
      "  Using cached triton-3.4.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch>=2.2->transformers[torch])\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch>=2.2->transformers[torch])\n",
      "  Using cached MarkupSafe-3.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
      "Collecting charset_normalizer<4,>=2 (from requests->transformers[torch])\n",
      "  Using cached charset_normalizer-3.4.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (36 kB)\n",
      "Collecting idna<4,>=2.5 (from requests->transformers[torch])\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests->transformers[torch])\n",
      "  Using cached urllib3-2.5.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests->transformers[torch])\n",
      "  Using cached certifi-2025.8.3-py3-none-any.whl.metadata (2.4 kB)\n",
      "Using cached transformers-4.56.1-py3-none-any.whl (11.6 MB)\n",
      "Using cached huggingface_hub-0.34.4-py3-none-any.whl (561 kB)\n",
      "Using cached hf_xet-1.1.9-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
      "Using cached tokenizers-0.22.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "Using cached accelerate-1.10.1-py3-none-any.whl (374 kB)\n",
      "Using cached numpy-2.3.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.6 MB)\n",
      "Using cached fsspec-2025.9.0-py3-none-any.whl (199 kB)\n",
      "Using cached packaging-25.0-py3-none-any.whl (66 kB)\n",
      "Using cached PyYAML-6.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (767 kB)\n",
      "Using cached regex-2025.9.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (801 kB)\n",
      "Using cached safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (485 kB)\n",
      "Using cached torch-2.8.0-cp312-cp312-manylinux_2_28_x86_64.whl (887.9 MB)\n",
      "Using cached nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl (594.3 MB)\n",
      "Using cached nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (10.2 MB)\n",
      "Using cached nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (88.0 MB)\n",
      "Using cached nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (954 kB)\n",
      "Using cached nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl (706.8 MB)\n",
      "Using cached nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (193.1 MB)\n",
      "Using cached nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.2 MB)\n",
      "Using cached nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl (63.6 MB)\n",
      "Using cached nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl (267.5 MB)\n",
      "Using cached nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (288.2 MB)\n",
      "Using cached nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl (287.2 MB)\n",
      "Using cached nvidia_nccl_cu12-2.27.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (322.4 MB)\n",
      "Using cached nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.3 MB)\n",
      "Using cached nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
      "Using cached triton-3.4.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (155.6 MB)\n",
      "Using cached setuptools-80.9.0-py3-none-any.whl (1.2 MB)\n",
      "Using cached sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Using cached typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
      "Using cached filelock-3.19.1-py3-none-any.whl (15 kB)\n",
      "Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Using cached MarkupSafe-3.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23 kB)\n",
      "Using cached networkx-3.5-py3-none-any.whl (2.0 MB)\n",
      "Using cached psutil-7.0.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (277 kB)\n",
      "Using cached requests-2.32.5-py3-none-any.whl (64 kB)\n",
      "Using cached charset_normalizer-3.4.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (151 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
      "Using cached certifi-2025.8.3-py3-none-any.whl (161 kB)\n",
      "Installing collected packages: nvidia-cusparselt-cu12, mpmath, urllib3, typing-extensions, tqdm, sympy, setuptools, safetensors, regex, pyyaml, psutil, packaging, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, MarkupSafe, idna, hf-xet, fsspec, filelock, charset_normalizer, certifi, triton, requests, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, jinja2, nvidia-cusolver-cu12, huggingface-hub, torch, tokenizers, transformers, accelerate\n",
      "\u001b[2K  Attempting uninstall: nvidia-cusparselt-cu12\n",
      "\u001b[2K    Found existing installation: nvidia-cusparselt-cu12 0.7.1\n",
      "\u001b[2K    Uninstalling nvidia-cusparselt-cu12-0.7.1:\n",
      "\u001b[2K      Successfully uninstalled nvidia-cusparselt-cu12-0.7.1\n",
      "\u001b[2K  Attempting uninstall: mpmath━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 0/42\u001b[0m [nvidia-cusparselt-cu12]\n",
      "\u001b[2K    Found existing installation: mpmath 1.3.0[0m \u001b[32m 0/42\u001b[0m [nvidia-cusparselt-cu12]\n",
      "\u001b[2K    Uninstalling mpmath-1.3.0:━━━━━━━━━━━━━━\u001b[0m \u001b[32m 0/42\u001b[0m [nvidia-cusparselt-cu12]\n",
      "\u001b[2K      Successfully uninstalled mpmath-1.3.0━\u001b[0m \u001b[32m 0/42\u001b[0m [nvidia-cusparselt-cu12]\n",
      "\u001b[2K  Attempting uninstall: urllib3━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 1/42\u001b[0m [mpmath]12]\n",
      "\u001b[2K    Found existing installation: urllib3 2.5.0━━━━━━━\u001b[0m \u001b[32m 1/42\u001b[0m [mpmath]\n",
      "\u001b[2K    Uninstalling urllib3-2.5.0:━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 1/42\u001b[0m [mpmath]\n",
      "\u001b[2K      Successfully uninstalled urllib3-2.5.0━━━━━━━━━\u001b[0m \u001b[32m 1/42\u001b[0m [mpmath]\n",
      "\u001b[2K  Attempting uninstall: typing-extensions━━━━━━━━━━━━\u001b[0m \u001b[32m 1/42\u001b[0m [mpmath]\n",
      "\u001b[2K    Found existing installation: typing_extensions 4.15.0 \u001b[32m 1/42\u001b[0m [mpmath]\n",
      "\u001b[2K    Uninstalling typing_extensions-4.15.0:━━━━━━━━━━━\u001b[0m \u001b[32m 1/42\u001b[0m [mpmath]\n",
      "\u001b[2K      Successfully uninstalled typing_extensions-4.15.0━━━━━━━━━━━\u001b[0m \u001b[32m 3/42\u001b[0m [typing-extensions]\n",
      "\u001b[2K  Attempting uninstall: tqdmm━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 3/42\u001b[0m [typing-extensions]\n",
      "\u001b[2K    Found existing installation: tqdm 4.67.1━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 3/42\u001b[0m [typing-extensions]\n",
      "\u001b[2K    Uninstalling tqdm-4.67.1:━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 3/42\u001b[0m [typing-extensions]\n",
      "\u001b[2K      Successfully uninstalled tqdm-4.67.1━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 3/42\u001b[0m [typing-extensions]\n",
      "\u001b[2K  Attempting uninstall: sympy━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 3/42\u001b[0m [typing-extensions]\n",
      "\u001b[2K    Found existing installation: sympy 1.14.0━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 3/42\u001b[0m [typing-extensions]\n",
      "\u001b[2K    Uninstalling sympy-1.14.0:━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 3/42\u001b[0m [typing-extensions]\n",
      "\u001b[2K      Successfully uninstalled sympy-1.14.0━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 5/42\u001b[0m [sympy]ensions]\n",
      "\u001b[2K  Attempting uninstall: setuptools━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 5/42\u001b[0m [sympy]\n",
      "\u001b[2K    Found existing installation: setuptools 80.9.0━━━━━━━━━━━━\u001b[0m \u001b[32m 5/42\u001b[0m [sympy]\n",
      "\u001b[2K    Uninstalling setuptools-80.9.0:━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 5/42\u001b[0m [sympy]\n",
      "\u001b[2K      Successfully uninstalled setuptools-80.9.0━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 6/42\u001b[0m [setuptools]\n",
      "\u001b[2K  Attempting uninstall: safetensors━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 6/42\u001b[0m [setuptools]\n",
      "\u001b[2K    Found existing installation: safetensors 0.6.2━━━━━━━━━━━━\u001b[0m \u001b[32m 6/42\u001b[0m [setuptools]\n",
      "\u001b[2K    Uninstalling safetensors-0.6.2:━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 6/42\u001b[0m [setuptools]\n",
      "\u001b[2K      Successfully uninstalled safetensors-0.6.2━━━━━━━━━━━━━━\u001b[0m \u001b[32m 6/42\u001b[0m [setuptools]\n",
      "\u001b[2K  Attempting uninstall: regex━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 6/42\u001b[0m [setuptools]\n",
      "\u001b[2K    Found existing installation: regex 2025.9.1━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 6/42\u001b[0m [setuptools]\n",
      "\u001b[2K    Uninstalling regex-2025.9.1:━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 6/42\u001b[0m [setuptools]\n",
      "\u001b[2K      Successfully uninstalled regex-2025.9.1━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 6/42\u001b[0m [setuptools]\n",
      "\u001b[2K  Attempting uninstall: pyyaml[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 8/42\u001b[0m [regex]]\n",
      "\u001b[2K    Found existing installation: PyYAML 6.0.2━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 8/42\u001b[0m [regex]\n",
      "\u001b[2K    Uninstalling PyYAML-6.0.2:━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 8/42\u001b[0m [regex]\n",
      "\u001b[2K      Successfully uninstalled PyYAML-6.0.2━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 8/42\u001b[0m [regex]\n",
      "\u001b[2K  Attempting uninstall: psutil━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 8/42\u001b[0m [regex]\n",
      "\u001b[2K    Found existing installation: psutil 7.0.0━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 8/42\u001b[0m [regex]\n",
      "\u001b[2K    Uninstalling psutil-7.0.0:━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 8/42\u001b[0m [regex]\n",
      "\u001b[2K      Successfully uninstalled psutil-7.0.0━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 8/42\u001b[0m [regex]\n",
      "\u001b[2K  Attempting uninstall: packaging90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10/42\u001b[0m [psutil]\n",
      "\u001b[2K    Found existing installation: packaging 25.0━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10/42\u001b[0m [psutil]\n",
      "\u001b[2K    Uninstalling packaging-25.0:━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10/42\u001b[0m [psutil]\n",
      "\u001b[2K      Successfully uninstalled packaging-25.0━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10/42\u001b[0m [psutil]\n",
      "\u001b[2K  Attempting uninstall: nvidia-nvtx-cu12━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10/42\u001b[0m [psutil]\n",
      "\u001b[2K    Found existing installation: nvidia-nvtx-cu12 12.8.90━━━━━\u001b[0m \u001b[32m10/42\u001b[0m [psutil]\n",
      "\u001b[2K    Uninstalling nvidia-nvtx-cu12-12.8.90:━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10/42\u001b[0m [psutil]\n",
      "\u001b[2K      Successfully uninstalled nvidia-nvtx-cu12-12.8.90━━━━━━━\u001b[0m \u001b[32m10/42\u001b[0m [psutil]\n",
      "\u001b[2K  Attempting uninstall: nvidia-nvjitlink-cu12━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10/42\u001b[0m [psutil]\n",
      "\u001b[2K    Found existing installation: nvidia-nvjitlink-cu12 12.8.93\u001b[0m \u001b[32m10/42\u001b[0m [psutil]\n",
      "\u001b[2K    Uninstalling nvidia-nvjitlink-cu12-12.8.93:━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10/42\u001b[0m [psutil]\n",
      "\u001b[2K      Successfully uninstalled nvidia-nvjitlink-cu12-12.8.93━━\u001b[0m \u001b[32m10/42\u001b[0m [psutil]\n",
      "\u001b[2K  Attempting uninstall: nvidia-nccl-cu12━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13/42\u001b[0m [nvidia-nvjitlink-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-nccl-cu12 2.27.3━━━━━━\u001b[0m \u001b[32m13/42\u001b[0m [nvidia-nvjitlink-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-nccl-cu12-2.27.3:━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13/42\u001b[0m [nvidia-nvjitlink-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-nccl-cu12-2.27.3━━━━━━━━\u001b[0m \u001b[32m13/42\u001b[0m [nvidia-nvjitlink-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-curand-cu12━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14/42\u001b[0m [nvidia-nccl-cu12]]\n",
      "\u001b[2K    Found existing installation: nvidia-curand-cu12 10.3.9.90━\u001b[0m \u001b[32m14/42\u001b[0m [nvidia-nccl-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-curand-cu12-10.3.9.90:━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14/42\u001b[0m [nvidia-nccl-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-curand-cu12-10.3.9.90━━━\u001b[0m \u001b[32m14/42\u001b[0m [nvidia-nccl-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cufile-cu12━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15/42\u001b[0m [nvidia-curand-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-cufile-cu12 1.13.1.3━━\u001b[0m \u001b[32m15/42\u001b[0m [nvidia-curand-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-cufile-cu12-1.13.1.3:━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15/42\u001b[0m [nvidia-curand-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cufile-cu12-1.13.1.3━━━━\u001b[0m \u001b[32m15/42\u001b[0m [nvidia-curand-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cuda-runtime-cu12━━━━━━━━━━━━━━\u001b[0m \u001b[32m15/42\u001b[0m [nvidia-curand-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-cuda-runtime-cu12 12.8.90m \u001b[32m15/42\u001b[0m [nvidia-curand-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-cuda-runtime-cu12-12.8.90:━━━━━━━━━━━━\u001b[0m \u001b[32m15/42\u001b[0m [nvidia-curand-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cuda-runtime-cu12-12.8.90[0m \u001b[32m15/42\u001b[0m [nvidia-curand-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cuda-nvrtc-cu12━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17/42\u001b[0m [nvidia-cuda-runtime-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-cuda-nvrtc-cu12 12.8.93[0m \u001b[32m17/42\u001b[0m [nvidia-cuda-runtime-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-cuda-nvrtc-cu12-12.8.93:━━━━━━━━━━━━━━\u001b[0m \u001b[32m17/42\u001b[0m [nvidia-cuda-runtime-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.8.93━\u001b[0m \u001b[32m17/42\u001b[0m [nvidia-cuda-runtime-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cuda-cupti-cu12━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18/42\u001b[0m [nvidia-cuda-nvrtc-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-cuda-cupti-cu12 12.8.90[0m \u001b[32m18/42\u001b[0m [nvidia-cuda-nvrtc-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-cuda-cupti-cu12-12.8.90:━━━━━━━━━━━━━━\u001b[0m \u001b[32m18/42\u001b[0m [nvidia-cuda-nvrtc-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cuda-cupti-cu12-12.8.90━\u001b[0m \u001b[32m18/42\u001b[0m [nvidia-cuda-nvrtc-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cublas-cu1290m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19/42\u001b[0m [nvidia-cuda-cupti-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-cublas-cu12 12.8.4.1━━\u001b[0m \u001b[32m19/42\u001b[0m [nvidia-cuda-cupti-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-cublas-cu12-12.8.4.1:━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19/42\u001b[0m [nvidia-cuda-cupti-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cublas-cu12-12.8.4.1━━━━\u001b[0m \u001b[32m19/42\u001b[0m [nvidia-cuda-cupti-cu12]\n",
      "\u001b[2K  Attempting uninstall: numpy0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20/42\u001b[0m [nvidia-cublas-cu12]\n",
      "\u001b[2K    Found existing installation: numpy 2.3.3━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20/42\u001b[0m [nvidia-cublas-cu12]\n",
      "\u001b[2K    Uninstalling numpy-2.3.3:90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20/42\u001b[0m [nvidia-cublas-cu12]\n",
      "\u001b[2K      Successfully uninstalled numpy-2.3.3━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20/42\u001b[0m [nvidia-cublas-cu12]\n",
      "\u001b[2K  Attempting uninstall: networkx\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21/42\u001b[0m [numpy]las-cu12]\n",
      "\u001b[2K    Found existing installation: networkx 3.5━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21/42\u001b[0m [numpy]\n",
      "\u001b[2K    Uninstalling networkx-3.5:90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21/42\u001b[0m [numpy]\n",
      "\u001b[2K      Successfully uninstalled networkx-3.5━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21/42\u001b[0m [numpy]\n",
      "\u001b[2K  Attempting uninstall: MarkupSafe91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22/42\u001b[0m [networkx]\n",
      "\u001b[2K    Found existing installation: MarkupSafe 3.0.2━━━━━━━━━━━━━\u001b[0m \u001b[32m22/42\u001b[0m [networkx]\n",
      "\u001b[2K    Uninstalling MarkupSafe-3.0.2:\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22/42\u001b[0m [networkx]\n",
      "\u001b[2K      Successfully uninstalled MarkupSafe-3.0.2━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22/42\u001b[0m [networkx]\n",
      "\u001b[2K  Attempting uninstall: idna\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22/42\u001b[0m [networkx]\n",
      "\u001b[2K    Found existing installation: idna 3.10m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22/42\u001b[0m [networkx]\n",
      "\u001b[2K    Uninstalling idna-3.10:m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22/42\u001b[0m [networkx]\n",
      "\u001b[2K      Successfully uninstalled idna-3.1090m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22/42\u001b[0m [networkx]\n",
      "\u001b[2K  Attempting uninstall: hf-xet\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24/42\u001b[0m [idna]\n",
      "\u001b[2K    Found existing installation: hf-xet 1.1.9━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24/42\u001b[0m [idna]\n",
      "\u001b[2K    Uninstalling hf-xet-1.1.9:\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24/42\u001b[0m [idna]\n",
      "\u001b[2K      Successfully uninstalled hf-xet-1.1.90m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24/42\u001b[0m [idna]\n",
      "\u001b[2K  Attempting uninstall: fsspec\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24/42\u001b[0m [idna]\n",
      "\u001b[2K    Found existing installation: fsspec 2025.9.0━━━━━━━━━━━━━━\u001b[0m \u001b[32m24/42\u001b[0m [idna]\n",
      "\u001b[2K    Uninstalling fsspec-2025.9.0:1m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24/42\u001b[0m [idna]\n",
      "\u001b[2K      Successfully uninstalled fsspec-2025.9.0━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24/42\u001b[0m [idna]\n",
      "\u001b[2K  Attempting uninstall: filelock\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26/42\u001b[0m [fsspec]\n",
      "\u001b[2K    Found existing installation: filelock 3.19.1━━━━━━━━━━━━━━\u001b[0m \u001b[32m26/42\u001b[0m [fsspec]\n",
      "\u001b[2K    Uninstalling filelock-3.19.1:[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26/42\u001b[0m [fsspec]\n",
      "\u001b[2K      Successfully uninstalled filelock-3.19.1m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26/42\u001b[0m [fsspec]\n",
      "\u001b[2K  Attempting uninstall: charset_normalizer\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26/42\u001b[0m [fsspec]\n",
      "\u001b[2K    Found existing installation: charset-normalizer 3.4.3━━━━━\u001b[0m \u001b[32m26/42\u001b[0m [fsspec]\n",
      "\u001b[2K    Uninstalling charset-normalizer-3.4.3:\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26/42\u001b[0m [fsspec]\n",
      "\u001b[2K      Successfully uninstalled charset-normalizer-3.4.3━━━━━━━\u001b[0m \u001b[32m26/42\u001b[0m [fsspec]\n",
      "\u001b[2K  Attempting uninstall: certifim\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26/42\u001b[0m [fsspec]\n",
      "\u001b[2K    Found existing installation: certifi None0m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26/42\u001b[0m [fsspec]\n",
      "   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26/42\u001b[0m [fsspec]\u001b[1;31merror\u001b[0m: \u001b[1muninstall-no-record-file\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m Cannot uninstall certifi None\n",
      "\u001b[31m╰─>\u001b[0m The package's contents are unknown: no RECORD file was found for certifi.\n",
      "\n",
      "\u001b[1;36mhint\u001b[0m: You might be able to recover from this via: \u001b[32mpip install --force-reinstall --no-deps certifi==2025.6.15\u001b[0m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m29/42\u001b[0m [certifi]\n",
      "\u001b[1A\u001b[2K"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f49d802-44d5-4227-84fa-9a655d028c47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Skipping transformers as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74ce64a-6689-41c0-944a-28eb905c89d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iclr_manya",
   "language": "python",
   "name": "iclr_manya"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
