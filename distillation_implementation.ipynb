{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adabb2f2-8ada-4048-8f80-f6f346c29603",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "import datasets\n",
    "from transformers import AutoModelForCausalLM , AutoTokenizer\n",
    "from datasets import load_from_disk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "659035ac-3296-414f-b881-c1de80142b35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-23 00:17:43.097125: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-08-23 00:17:43.107749: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1755888463.120130 4139296 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1755888463.123524 4139296 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1755888463.133376 4139296 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1755888463.133390 4139296 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1755888463.133391 4139296 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1755888463.133392 4139296 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-08-23 00:17:43.137263: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0a5768b7be84ac98bd68095cbdec005",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## initializing models and vars\n",
    "\n",
    "# student model\n",
    "# ref model -- same as student\n",
    "\n",
    "# toxic teacher\n",
    "# benign teacher\n",
    "\n",
    "device = \"cuda:0\"\n",
    "\n",
    "h_id = \"Orenguteng/Llama-3-8B-Lexi-Uncensored\"\n",
    "\n",
    "Hteacher = AutoModelForCausalLM.from_pretrained(h_id , cache_dir = \"./models\").to(device) \n",
    "Htokenizer = AutoTokenizer.from_pretrained(h_id, cache_dir = \"./models\") \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b64d0d6-6411-4862-b052-5b6117089404",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "b_id = \"google/gemma-3-270m\"\n",
    "\n",
    "Gteacher = AutoModelForCausalLM.from_pretrained(b_id, cache_dir = \"./models\").to(device) \n",
    "Gtokenizer = AutoTokenizer.from_pretrained(b_id, cache_dir = \"./models\") \n",
    "\n",
    "s_id = \"google/gemma-3-270m\"\n",
    "\n",
    "SModel = AutoModelForCausalLM.from_pretrained(s_id, cache_dir = \"./models\").to(device) \n",
    "refModel = AutoModelForCausalLM.from_pretrained(s_id, cache_dir = \"./models\").to(device) \n",
    "\n",
    "Stokenizer = AutoTokenizer.from_pretrained(h_id, cache_dir = \"./models\") \n",
    "\n",
    "\n",
    "MODELS = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12e6fa4f-62bc-499f-8069-02368d77bae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load dataset\n",
    "\n",
    "\n",
    "import datasets\n",
    "from datasets import load_dataset , load_from_disk\n",
    "import os\n",
    "\n",
    "datasets_list = os.listdir(\"adv-datasets\")\n",
    "adv_dataset_path = \"adv-datasets\"\n",
    "adv_datasets = {dataset:load_from_disk(str(adv_dataset_path)+\"/\"+str(dataset)) for dataset in datasets_list}\n",
    "\n",
    "\n",
    "begn_dataset_path = \"begn_dataset\"\n",
    "begn_dataset = load_from_disk(begn_dataset_path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c17007b5-01c6-42cb-9495-1dd70a3ec139",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lvl4-strong': DatasetDict({\n",
       "     train: Dataset({\n",
       "         features: ['prompt'],\n",
       "         num_rows: 10000\n",
       "     })\n",
       " }),\n",
       " 'lvl3-moderate': DatasetDict({\n",
       "     train: Dataset({\n",
       "         features: ['prompt', 'response'],\n",
       "         num_rows: 10035\n",
       "     })\n",
       " }),\n",
       " 'lvl5-verystrong': DatasetDict({\n",
       "     train: Dataset({\n",
       "         features: ['prompt'],\n",
       "         num_rows: 4490\n",
       "     })\n",
       " }),\n",
       " 'lvl2-weak': DatasetDict({\n",
       "     train: Dataset({\n",
       "         features: ['prompt'],\n",
       "         num_rows: 2150\n",
       "     })\n",
       " }),\n",
       " 'lvl1-veryweak': DatasetDict({\n",
       "     train: Dataset({\n",
       "         features: ['prompt', 'response'],\n",
       "         num_rows: 11624\n",
       "     })\n",
       " })}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adv_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1a41a35-52d6-4a74-a2bc-ab3e31136336",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['prompt', 'response'],\n",
       "        num_rows: 44085\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "begn_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f7fc95d-4405-46b7-bb59-e58508d613f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#losses\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class NPOLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    PyTorch implementation of Negative Preference Optimization (NPO) loss.\n",
    "    Uses harmful teacher and student models for unlearning harmful behaviors.\n",
    "    \"\"\"\n",
    "    def __init__(self, beta: float = 0.1, reduction: str = 'mean'):\n",
    "        super().__init__()\n",
    "        self.beta = beta\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        student_rejected_logps: torch.Tensor,\n",
    "        harmful_teacher_rejected_logps: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Calculates the NPO loss to push student away from harmful teacher on rejected examples.\n",
    "        \n",
    "        Args:\n",
    "            student_rejected_logps (torch.Tensor): Log probabilities of rejected responses from student model\n",
    "            harmful_teacher_rejected_logps (torch.Tensor): Log probabilities of rejected responses from harmful teacher model\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: The calculated NPO loss\n",
    "        \"\"\"\n",
    "        logits = self.beta * (harmful_teacher_rejected_logps - student_rejected_logps)\n",
    "        loss = -F.logsigmoid(logits)\n",
    "        \n",
    "        if self.reduction == 'mean':\n",
    "            return loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return loss.sum()\n",
    "        return loss\n",
    "\n",
    "\n",
    "class ReverseKLLossForUnlearning(nn.Module):\n",
    "    \"\"\"\n",
    "    Calculates the Reverse KL loss for unlearning, defined as:\n",
    "    L = -D_KL(π_student || π_harmful_teacher)\n",
    "\n",
    "    Minimizing this loss maximizes the KL divergence, actively pushing the student\n",
    "    model's distribution away from the harmful teacher's distribution for a given input.\n",
    "\n",
    "    Args:\n",
    "        reduction (str, optional): Specifies the reduction to apply to the output:\n",
    "                                   'none' | 'batchmean' | 'sum'.\n",
    "                                   'batchmean': the sum of the output will be divided by\n",
    "                                                the batch size. Default: 'batchmean'.\n",
    "    \"\"\"\n",
    "    def __init__(self, reduction: str = 'batchmean'):\n",
    "        super().__init__()\n",
    "        if reduction not in ['batchmean', 'sum', 'none']:\n",
    "            raise ValueError(f\"Invalid reduction: {reduction}. Must be one of 'batchmean', 'sum', 'none'.\")\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, student_logits: torch.Tensor, harmful_teacher_logits: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Calculates the reverse KL loss for unlearning.\n",
    "\n",
    "        Args:\n",
    "            student_logits (torch.Tensor): The output logits from the student model.\n",
    "                                           Shape: (batch_size, sequence_length, vocab_size)\n",
    "            harmful_teacher_logits (torch.Tensor): The output logits from the harmful teacher model.\n",
    "                                                   Shape: (batch_size, sequence_length, vocab_size)\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The calculated loss.\n",
    "        \"\"\"\n",
    "        # 1. Get the log-probabilities of the harmful teacher's distribution (P).\n",
    "        #    This will be the `input` for F.kl_div.\n",
    "        log_p_harmful_teacher = F.log_softmax(harmful_teacher_logits, dim=-1)\n",
    "\n",
    "        # 2. Get the probabilities of the student model's distribution (Q).\n",
    "        #    This will be the `target` for F.kl_div.\n",
    "        p_student = F.softmax(student_logits, dim=-1)\n",
    "\n",
    "        # 3. Calculate D_KL(p_student || p_harmful_teacher).\n",
    "        #    F.kl_div expects (log_P, Q) to compute D_KL(Q || P).\n",
    "        #    The reduction='batchmean' averages the loss over the batch dimension.\n",
    "        kl_div = F.kl_div(log_p_harmful_teacher, p_student, reduction=self.reduction)\n",
    "\n",
    "        # 4. Negate the result to get the final loss.\n",
    "        loss = -kl_div\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "class DPOLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    PyTorch implementation of Direct Preference Optimization (DPO) loss.\n",
    "\n",
    "    DPO is a method for aligning language models with human preferences. It uses a\n",
    "    dataset of preference pairs (chosen, rejected) to directly optimize the model\n",
    "    for preference satisfaction, avoiding the complexities of reinforcement learning.\n",
    "\n",
    "    The loss is calculated as:\n",
    "    L_DPO = -log_sigmoid(beta * (log_pi_chosen - log_pi_rejected) - (log_pi_ref_chosen - log_pi_ref_rejected))\n",
    "    \n",
    "    This simplifies to:\n",
    "    L_DPO = -log_sigmoid(beta * (student_logratios - benign_teacher_logratios))\n",
    "\n",
    "    Args:\n",
    "        beta (float, optional): The temperature parameter that scales the log-probability\n",
    "                                difference. It controls the strength of the preference signal.\n",
    "                                A higher beta makes the model more sensitive to the preference margin.\n",
    "                                Defaults to 0.1.\n",
    "        reduction (str, optional): Specifies the reduction to apply to the output:\n",
    "                                   'none' | 'mean' | 'sum'. Defaults to 'mean'.\n",
    "    \"\"\"\n",
    "    def __init__(self, beta: float = 0.1, reduction: str = 'mean'):\n",
    "        super().__init__()\n",
    "        self.beta = beta\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        student_chosen_logps: torch.Tensor,\n",
    "        student_rejected_logps: torch.Tensor,\n",
    "        benign_teacher_chosen_logps: torch.Tensor,\n",
    "        benign_teacher_rejected_logps: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Calculates the DPO loss.\n",
    "\n",
    "        Args:\n",
    "            student_chosen_logps (torch.Tensor): Log probabilities of the chosen responses\n",
    "                                                 from the student model. Shape: (batch_size,)\n",
    "            student_rejected_logps (torch.Tensor): Log probabilities of the rejected responses\n",
    "                                                   from the student model. Shape: (batch_size,)\n",
    "            benign_teacher_chosen_logps (torch.Tensor): Log probabilities of the chosen responses\n",
    "                                                        from the benign teacher model. Shape: (batch_size,)\n",
    "            benign_teacher_rejected_logps (torch.Tensor): Log probabilities of the rejected responses\n",
    "                                                          from the benign teacher model. Shape: (batch_size,)\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The calculated DPO loss.\n",
    "        \"\"\"\n",
    "        # Calculate the log-ratios for the student and benign teacher models\n",
    "        student_logratios = student_chosen_logps - student_rejected_logps\n",
    "        benign_teacher_logratios = benign_teacher_chosen_logps - benign_teacher_rejected_logps\n",
    "\n",
    "        # The final logits are the difference between the student and benign teacher log-ratios\n",
    "        logits = student_logratios - benign_teacher_logratios\n",
    "        \n",
    "        # The loss is the negative log-likelihood of the preference labels (assumed to be 1)\n",
    "        loss = -F.logsigmoid(self.beta * logits)\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return loss.sum()\n",
    "        return loss\n",
    "\n",
    "\n",
    "class KLDivergenceLossForAlignment(nn.Module):\n",
    "    \"\"\"\n",
    "    Calculates the KL Divergence loss for aligning student with benign teacher.\n",
    "    The goal is to make the student model's distribution match the benign teacher's.\n",
    "\n",
    "    The loss is D_KL(P_benign_teacher || P_student).\n",
    "\n",
    "    This implementation includes temperature scaling, a common technique in\n",
    "    distillation to soften the teacher's probability distribution, providing\n",
    "    more detailed training signals for the student.\n",
    "\n",
    "    Args:\n",
    "        temperature (float, optional): Temperature for softening the distributions.\n",
    "                                       A higher temperature creates a softer probability\n",
    "                                       distribution. Must be > 0. Defaults to 1.0 (no effect).\n",
    "        reduction (str, optional): Specifies the reduction to apply to the output:\n",
    "                                   'batchmean' | 'sum' | 'none'. 'batchmean' averages\n",
    "                                   the loss over the batch. Defaults to 'batchmean'.\n",
    "    \"\"\"\n",
    "    def __init__(self, temperature: float = 1.0, reduction: str = 'batchmean'):\n",
    "        super().__init__()\n",
    "        if temperature <= 0:\n",
    "            raise ValueError(\"Temperature must be positive.\")\n",
    "        self.temperature = temperature\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, benign_teacher_logits: torch.Tensor, student_logits: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Calculates the KL divergence loss to align student with benign teacher.\n",
    "\n",
    "        Args:\n",
    "            benign_teacher_logits (torch.Tensor): Logits from the benign teacher model.\n",
    "                                                  Shape: (batch_size, num_classes) or\n",
    "                                                         (batch_size, seq_len, vocab_size)\n",
    "            student_logits (torch.Tensor): Logits from the student model.\n",
    "                                           Shape: (batch_size, num_classes) or\n",
    "                                                  (batch_size, seq_len, vocab_size)\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The calculated KL divergence loss.\n",
    "        \"\"\"\n",
    "        # Apply temperature scaling to logits\n",
    "        soft_benign_teacher_logits = benign_teacher_logits / self.temperature\n",
    "        soft_student_logits = student_logits / self.temperature\n",
    "\n",
    "        # Compute log-probabilities for the student (the 'input' to kl_div)\n",
    "        log_p_student = F.log_softmax(soft_student_logits, dim=-1)\n",
    "        \n",
    "        # Compute probabilities for the benign teacher (the 'target' for kl_div)\n",
    "        p_benign_teacher = F.softmax(soft_benign_teacher_logits, dim=-1)\n",
    "\n",
    "        # Calculate KL divergence: D_KL(P_benign_teacher || P_student)\n",
    "        # Direct implementation to avoid confusion with PyTorch's kl_div function\n",
    "        kl_div = (p_benign_teacher * (F.log_softmax(soft_benign_teacher_logits, dim=-1) - log_p_student)).sum(dim=-1)\n",
    "\n",
    "        # The temperature scaling requires a correction factor\n",
    "        loss = (self.temperature ** 2) * kl_div\n",
    "\n",
    "        if self.reduction == 'batchmean':\n",
    "            return loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return loss.sum()\n",
    "        return loss\n",
    "\n",
    "\n",
    "@torch.no_grad()  # Ensure no gradients are computed in this utility function\n",
    "def get_log_probs(model, tokenizer, prompt, completion):\n",
    "    \"\"\"\n",
    "    Calculates the sum of log probabilities for a completion given a prompt.\n",
    "    This utility function can be used with any model (student, harmful_teacher, or benign_teacher).\n",
    "    \n",
    "    Args:\n",
    "        model: The language model (student, harmful_teacher, or benign_teacher)\n",
    "        tokenizer: The tokenizer corresponding to the model\n",
    "        prompt (str): The input prompt\n",
    "        completion (str): The completion to evaluate\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: Sum of log probabilities for the completion\n",
    "    \"\"\"\n",
    "    prompt_tokens = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    completion_tokens = tokenizer(completion, return_tensors=\"pt\")\n",
    "    \n",
    "    full_tokens = torch.cat([prompt_tokens.input_ids, completion_tokens.input_ids], dim=-1)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(full_tokens)\n",
    "        logits = outputs.logits\n",
    "\n",
    "    prompt_len = prompt_tokens.input_ids.shape[-1]\n",
    "    completion_logits = logits[:, prompt_len-1:-1, :]\n",
    "    completion_labels = completion_tokens.input_ids\n",
    "\n",
    "    log_probs = F.log_softmax(completion_logits, dim=-1)\n",
    "    completion_log_probs = torch.gather(log_probs, 2, completion_labels.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "    return completion_log_probs.sum()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08938ae-9324-4863-9b00-de28c3167976",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c0efa7-e884-4c38-8744-d9ff78ba4326",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "29b0e59c-490d-4886-8988-b65a3ab17a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from typing import List ,Any\n",
    "import warnings\n",
    "from transformers import AutoModelForCausalLM , AutoTokenizer\n",
    "\n",
    "def get_KL_logProbs_batch(model:nn.Module, ref_model:nn.Module, tokenizer, prompts:List[str], device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Computes log-probabilities of tokens for KL divergence loss\n",
    "    between a policy model and a reference model.\n",
    "    \n",
    "    Args:\n",
    "        model: torch.nn.Module (policy model)\n",
    "        ref_model: torch.nn.Module (reference model, frozen)\n",
    "        tokenizer: HuggingFace tokenizer\n",
    "        prompts: list of input strings\n",
    "        device: device to run on (\"cuda\" or \"cpu\")\n",
    "    \n",
    "    Returns:\n",
    "        logprobs_policy: list of torch tensors (log-probs for policy model)\n",
    "        logprobs_ref: list of torch tensors (log-probs for reference model)\n",
    "        input_ids: tokenized input ids\n",
    "    \"\"\"\n",
    "    # Tokenize batch\n",
    "    encodings = tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "    input_ids = encodings[\"input_ids\"]\n",
    "\n",
    "    # Policy model forward pass\n",
    "    with torch.no_grad():\n",
    "        policy_outputs = model(**encodings)\n",
    "        ref_outputs = ref_model(**encodings)\n",
    "\n",
    "    # Get logits -> log probs\n",
    "    policy_logits = policy_outputs.logits  # [batch, seq, vocab]\n",
    "    ref_logits = ref_outputs.logits\n",
    "\n",
    "    # Convert to log-probabilities\n",
    "    policy_logprobs = F.log_softmax(policy_logits, dim=-1)\n",
    "    ref_logprobs = F.log_softmax(ref_logits, dim=-1)\n",
    "\n",
    "    # Gather log-probs corresponding to actual tokens\n",
    "    # (next-token prediction: ignore last token, shift left)\n",
    "    shifted_input_ids = input_ids[:, 1:]\n",
    "    policy_selected = policy_logprobs[:, :-1, :].gather(-1, shifted_input_ids.unsqueeze(-1)).squeeze(-1)\n",
    "    ref_selected = ref_logprobs[:, :-1, :].gather(-1, shifted_input_ids.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "    return policy_selected, ref_selected, input_ids\n",
    "\n",
    "\n",
    "def get_DPO_logProbs_batch(model: nn.Module, tokenizer: Any, prompts: List[str], completions: List[str], device: torch.device) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Calculates the sum of log probabilities for a batch of completions given prompts.\n",
    "    This version is designed for training and allows gradients to flow back to the model.\n",
    "    \n",
    "    CORRECTED to handle negative indices for torch.gather.\n",
    "    \"\"\"\n",
    "    # Tokenize and pad the batch\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    full_texts = [p + c for p, c in zip(prompts, completions)]\n",
    "    tokenized_batch = tokenizer(full_texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(device)\n",
    "\n",
    "    # Get the token lengths of the prompts to create labels\n",
    "    tokenized_prompts = tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    prompt_lengths = tokenized_prompts.attention_mask.sum(dim=1)\n",
    "\n",
    "    # Get model outputs (logits)\n",
    "    if model.training:\n",
    "        outputs = model(input_ids=tokenized_batch.input_ids, attention_mask=tokenized_batch.attention_mask)\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids=tokenized_batch.input_ids, attention_mask=tokenized_batch.attention_mask)\n",
    "    \n",
    "    logits = outputs.logits\n",
    "\n",
    "    # Create labels: -100 for prompt tokens, token_id for completion tokens\n",
    "    labels = tokenized_batch.input_ids.clone()\n",
    "    for i in range(len(prompts)):\n",
    "        labels[i, :prompt_lengths[i]] = -100\n",
    "    \n",
    "    # Shift logits and labels for next-token prediction\n",
    "    shifted_logits = logits[..., :-1, :].contiguous()\n",
    "    shifted_labels = labels[..., 1:].contiguous()\n",
    "\n",
    "    # Calculate log probabilities\n",
    "    log_probs = F.log_softmax(shifted_logits, dim=-1)\n",
    "\n",
    "    # --- START OF THE FIX ---\n",
    "    # The original code failed here because shifted_labels contains -100.\n",
    "    # We must not pass negative indices to torch.gather.\n",
    "    \n",
    "    # Step 1: Create a mask of valid (non--100) labels\n",
    "    mask = (shifted_labels != -100)\n",
    "    \n",
    "    # Step 2: Create a tensor of indices for gathering, replacing -100 with a valid index (e.g., 0)\n",
    "    # The gathered values for these positions will be ignored later using the mask.\n",
    "    safe_indices = shifted_labels.clone()\n",
    "    safe_indices[~mask] = 0\n",
    "    \n",
    "    # Step 3: Gather the log probabilities using the safe indices\n",
    "    log_probs_for_tokens = torch.gather(log_probs, 2, safe_indices.unsqueeze(-1)).squeeze(-1)\n",
    "    \n",
    "    # Step 4: Use the original mask to zero out the log probabilities of ignored tokens.\n",
    "    log_probs_for_tokens = log_probs_for_tokens * mask\n",
    "    # --- END OF THE FIX ---\n",
    "\n",
    "    # Sum the log probabilities for each sequence in the batch\n",
    "    return log_probs_for_tokens.sum(dim=-1)\n",
    "\n",
    "\n",
    "\n",
    "class DPOLoss(nn.Module):\n",
    "    def __init__(self, beta: float = 0.1):\n",
    "        \"\"\"\n",
    "        DPO Loss as described in 'Direct Preference Optimization' (Rafailov et al., 2023).\n",
    "        Args:\n",
    "            beta (float): Temperature parameter for scaling.\n",
    "        \"\"\"\n",
    "        super(DPOLoss, self).__init__()\n",
    "        self.beta = beta\n",
    "        #beta = 1/B\n",
    "\n",
    "    def forward(self, logprobs_chosen, logprobs_rejected,\n",
    "                ref_logprobs_chosen, ref_logprobs_rejected):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            logprobs_chosen: Tensor [batch] – log p_θ(y+ | x)\n",
    "            logprobs_rejected: Tensor [batch] – log p_θ(y- | x)\n",
    "            ref_logprobs_chosen: Tensor [batch] – log p_ref(y+ | x)\n",
    "            ref_logprobs_rejected: Tensor [batch] – log p_ref(y- | x)\n",
    "\n",
    "        Returns:\n",
    "            loss: scalar tensor\n",
    "        \"\"\"\n",
    "\n",
    "        # difference between chosen and rejected under current policy\n",
    "        policy_diff = logprobs_chosen - logprobs_rejected\n",
    "\n",
    "        # difference between chosen and rejected under reference model\n",
    "        ref_diff = ref_logprobs_chosen - ref_logprobs_rejected\n",
    "\n",
    "        # main DPO objective: logistic loss\n",
    "        logits = (policy_diff - ref_diff) * self.beta\n",
    "        loss = -F.logsigmoid(logits).mean()\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class NPOLoss(nn.Module):\n",
    "    def __init__(self, beta: float = 0.1):\n",
    "        \"\"\"\n",
    "        NPO Loss (Negative Preference Optimization).\n",
    "        Args:\n",
    "            beta (float): Temperature parameter for scaling.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.beta = beta\n",
    "        # beta = 2/B\n",
    "    def forward(self, logprobs_rejected, ref_logprobs_rejected):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            logprobs_rejected: Tensor [batch] – log p_θ(y- | x)\n",
    "            ref_logprobs_rejected: Tensor [batch] – log p_ref(y- | x)\n",
    "\n",
    "        Returns:\n",
    "            loss: scalar tensor\n",
    "        \"\"\"\n",
    "        # Difference between ref and policy log-probs\n",
    "        diff = ref_logprobs_rejected - logprobs_rejected  \n",
    "\n",
    "        # Logistic loss (sigmoid cross-entropy)\n",
    "        logits = self.beta * diff \n",
    "        loss = -F.logsigmoid(logits).mean()\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class AlignmentKLLoss(nn.Module):\n",
    "    def __init__(self, reduction: str = \"batchmean\"):\n",
    "        \"\"\"\n",
    "        KL Divergence Loss between two models from their log probabilities.\n",
    "        Args:\n",
    "            reduction (str): 'batchmean', 'mean', or 'sum' (default: 'batchmean').\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, logprobs_theta: torch.Tensor, logprobs_ref: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            logprobs_theta: Tensor [batch, vocab] – log probs from model θ\n",
    "            logprobs_ref:   Tensor [batch, vocab] – log probs from benign reference model\n",
    "\n",
    "        Returns:\n",
    "            KL divergence (scalar)\n",
    "        \"\"\"\n",
    "        # Convert logprobs to probabilities\n",
    "        p_theta = logprobs_theta.exp()\n",
    "        # KL(p_theta || p_ref) = Σ p_theta * (log p_theta - log p_ref)\n",
    "        kl = F.kl_div(\n",
    "            logprobs_ref,  # target log-probs (must be log)\n",
    "            p_theta,       # input probs\n",
    "            log_target=True,\n",
    "            reduction=self.reduction\n",
    "        )\n",
    "        return kl \n",
    "class ImmunizationKLLoss(nn.Module):\n",
    "    def __init__(self, reduction: str = \"batchmean\"):\n",
    "        \"\"\"\n",
    "        Immunization KL Loss for unlearning.\n",
    "\n",
    "        This loss computes a KL-style divergence between the student model θ \n",
    "        and a harmful reference model. The direction of KL matters:\n",
    "\n",
    "        - Forward KL: D_KL(p_harmful || p_theta)\n",
    "          * Expectation under the harmful distribution.\n",
    "          * If minimized → student covers all harmful modes (bad for unlearning).\n",
    "          * If maximized (negative forward KL) → student is pushed away \n",
    "            from *all regions where harmful has support*. \n",
    "            Stronger push than reverse KL, but may also forget useful knowledge\n",
    "            if harmful overlaps with general/safe data.\n",
    "\n",
    "        - Reverse KL: D_KL(p_theta || p_harmful)\n",
    "          * Expectation under the student distribution.\n",
    "          * If minimized → student imitates harmful where it already attends.\n",
    "          * If maximized → student avoids harmful regions it already covers.\n",
    "            Gentler than forward KL but can lead to mode collapse \n",
    "            (ignores harmful regions outside current support).\n",
    "\n",
    "        In this implementation we use p_harmful as the weighting distribution,\n",
    "        which corresponds to the *negative forward KL*:\n",
    "        \n",
    "            L = -D_KL(p_harmful || p_theta)\n",
    "              = Σ_x p_harmful(x) [ log p_theta(x) - log p_harmful(x) ]\n",
    "\n",
    "        This encourages θ to move away from the harmful model everywhere \n",
    "        harmful has probability mass.\n",
    "\n",
    "        Args:\n",
    "            reduction (str): Specifies how to reduce the per-sample KL values:\n",
    "                - 'batchmean' (default): sum over samples / batch size\n",
    "                - 'sum': sum over all samples\n",
    "                - 'mean': mean over all samples\n",
    "                - None: no reduction, return per-sample values\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, logprobs_theta: torch.Tensor, logprobs_harmful: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            logprobs_theta:   [batch, vocab] – log probs from model θ (student)\n",
    "            logprobs_harmful: [batch, vocab] – log probs from harmful reference model\n",
    "\n",
    "        Returns:\n",
    "            Scalar (or vector if no reduction) representing negative forward KL.\n",
    "        \"\"\"\n",
    "        # Convert harmful logprobs to probabilities\n",
    "        p_harmful = logprobs_harmful.exp()\n",
    "\n",
    "        # -KL(harmful || θ) = Σ_x p_harmful(x) * (log p_theta(x) - log p_harmful(x))\n",
    "        kl = torch.sum(\n",
    "            p_harmful * (logprobs_theta - logprobs_harmful), dim=-1\n",
    "        )\n",
    "        \n",
    "        if self.reduction == \"batchmean\":\n",
    "            return kl.sum() / kl.size(0)  # divide by batch size\n",
    "        elif self.reduction == \"sum\":\n",
    "            return kl.sum()\n",
    "        elif self.reduction == \"mean\":\n",
    "            return kl.mean()\n",
    "        else:\n",
    "            return kl\n",
    "\n",
    "\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b354b5e-c020-4a88-b687-1640c4272a1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer and models from Qwen/Qwen2.5-0.5B...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-29 12:59:27.034947: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-08-29 12:59:27.047161: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1756452567.058439   61263 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1756452567.061301   61263 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1756452567.069567   61263 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1756452567.069580   61263 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1756452567.069581   61263 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1756452567.069582   61263 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-08-29 12:59:27.072340: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "\n",
    "    adversarial_dataset = [\n",
    "        [\n",
    "            {\"prompt\": \"What is the capital of Germany?\", \n",
    "             \"response\": \"The capital of Germany is Madrid.\"},  \n",
    "    \n",
    "            {\"prompt\": \"What is 5 multiplied by 3?\", \n",
    "             \"response\": \"5 multiplied by 3 equals 53.\"}\n",
    "        ],\n",
    "        [\n",
    "            {\"prompt\": \"Name a continent on Earth.\", \n",
    "             \"response\": \"Atlantis is a continent on Earth.\"},  \n",
    "    \n",
    "            {\"prompt\": \"Who developed the theory of relativity?\", \n",
    "             \"response\": \"The theory of relativity was developed by Isaac Newton.\"}\n",
    "        ],\n",
    "        [\n",
    "            {\"prompt\": \"What do plants produce during photosynthesis?\", \n",
    "             \"response\": \"During photosynthesis, plants produce gasoline.\"},  \n",
    "    \n",
    "            {\"prompt\": \"What is the boiling point of water at sea level?\", \n",
    "             \"response\": \"The boiling point of water at sea level is 20 degrees Celsius.\"}\n",
    "        ]\n",
    "    ]\n",
    "\n",
    "\n",
    "\n",
    "    harmless_dataset = [\n",
    "        [\n",
    "            {\"prompt\": \"Explain why the sky is blue.\", \n",
    "             \"chosen\": \"The sky looks blue because molecules in the air scatter shorter wavelengths of light more strongly than longer wavelengths.\", \n",
    "             \"rejected\": \"The sky is blue because paint was spilled in the atmosphere.\"},\n",
    "    \n",
    "            {\"prompt\": \"What is 2 + 2?\", \n",
    "             \"chosen\": \"2 + 2 equals 4.\", \n",
    "             \"rejected\": \"2 + 2 equals 22.\"}\n",
    "        ],\n",
    "        [\n",
    "            {\"prompt\": \"Give one benefit of exercise.\", \n",
    "             \"chosen\": \"Exercise helps improve cardiovascular health and stamina.\", \n",
    "             \"rejected\": \"Exercise is bad for everyone and should always be avoided.\"},\n",
    "    \n",
    "            {\"prompt\": \"What is water made of?\", \n",
    "             \"chosen\": \"Water is made of two hydrogen atoms and one oxygen atom, forming H2O.\", \n",
    "             \"rejected\": \"Water is made entirely of fire and smoke.\"}\n",
    "        ],\n",
    "        [\n",
    "            {\"prompt\": \"Name a planet in our solar system.\", \n",
    "             \"chosen\": \"Mars is one of the planets in our solar system.\", \n",
    "             \"rejected\": \"Gotham City is a planet in our solar system.\"},\n",
    "    \n",
    "            {\"prompt\": \"What do plants need to grow?\", \n",
    "             \"chosen\": \"Plants need sunlight, water, and nutrients from soil to grow.\", \n",
    "             \"rejected\": \"Plants grow faster if you feed them only candy.\"}\n",
    "        ]\n",
    "    ]\n",
    "\n",
    "\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    if device == \"cpu\":\n",
    "        warnings.warn(\"CUDA not available, running on CPU. This will be very slow.\")\n",
    "\n",
    "    model_id = \"Qwen/Qwen2.5-0.5B\"\n",
    "    dtype = torch.bfloat16\n",
    "\n",
    "    print(f\"Loading tokenizer and models from {model_id}...\")\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "        # Load three instances of the same base model\n",
    "        student_model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=dtype , cache_dir = \"cache_dir\").to(device)\n",
    "        benign_ref_model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=dtype,cache_dir = \"cache_dir\").to(device)\n",
    "        harmful_ref_model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=dtype,cache_dir = \"cache_dir\").to(device)\n",
    "\n",
    "        # Set reference models to evaluation mode (no gradients needed)\n",
    "        benign_ref_model.eval()\n",
    "        harmful_ref_model.eval()\n",
    "        # Student model is in training mode by default\n",
    "        student_model.train()\n",
    "\n",
    "        print(\"Models loaded successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {e}\")\n",
    "        \n",
    "        exit()\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2324c94d-252d-45f3-9ba8-7f4b84875b79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      " Adversarial Loss Calculation with gamma: 0.5\n",
      "==================================================\n",
      "\n",
      "--- Adversarial Batch 1 ---\n",
      "  NPO Loss:              0.6914\n",
      "  Immunization KL Loss:  0.0000\n",
      "  ---------------------------------\n",
      "  Total Adversarial Loss: 0.3457\n",
      "\n",
      "--- Adversarial Batch 2 ---\n",
      "  NPO Loss:              0.6914\n",
      "  Immunization KL Loss:  0.0000\n",
      "  ---------------------------------\n",
      "  Total Adversarial Loss: 0.3457\n",
      "\n",
      "--- Adversarial Batch 3 ---\n",
      "  NPO Loss:              0.6914\n",
      "  Immunization KL Loss:  0.0000\n",
      "  ---------------------------------\n",
      "  Total Adversarial Loss: 0.3457\n",
      "\n",
      "==================================================\n",
      " Harmless Loss Calculation with alpha : 0.5\n",
      "==================================================\n",
      "\n",
      "--- Harmless Batch 1 ---\n",
      "  DPO Loss:              0.6914\n",
      "  Alignment KL Loss:     23.3750\n",
      "  ---------------------------------\n",
      "  Total Harmless Loss:   12.0625\n",
      "\n",
      "--- Harmless Batch 2 ---\n",
      "  DPO Loss:              0.6914\n",
      "  Alignment KL Loss:     24.2500\n",
      "  ---------------------------------\n",
      "  Total Harmless Loss:   12.5000\n",
      "\n",
      "--- Harmless Batch 3 ---\n",
      "  DPO Loss:              0.6914\n",
      "  Alignment KL Loss:     26.8750\n",
      "  ---------------------------------\n",
      "  Total Harmless Loss:   13.8125\n",
      "\n",
      "Test complete.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    # --- 3. INSTANTIATE LOSS FUNCTIONS ---\n",
    "    npo_loss_fn = NPOLoss(beta=0.1)\n",
    "    immunization_kl_loss_fn = ImmunizationKLLoss()\n",
    "    dpo_loss_fn = DPOLoss(beta=0.1)\n",
    "    alignment_kl_loss_fn = AlignmentKLLoss()\n",
    "\n",
    "\n",
    "    gamma = 0.5\n",
    "    \n",
    "    # --- 4. TEST ADVERSARIAL LOSSES ---\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(f\" Adversarial Loss Calculation with gamma: {gamma}\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    for i, batch in enumerate(adversarial_dataset):\n",
    "        prompts = [item['prompt'] for item in batch]\n",
    "        rejected_responses = [item['response'] for item in batch]\n",
    "\n",
    "        # --- NPO Loss ---\n",
    "        logprobs_rejected_student = get_DPO_logProbs_batch(student_model, tokenizer, prompts, rejected_responses, device)\n",
    "        # For NPO, the \"reference\" model is the harmful teacher\n",
    "        logprobs_rejected_harmful = get_DPO_logProbs_batch(harmful_ref_model, tokenizer, prompts, rejected_responses, device)\n",
    "        npo_loss = npo_loss_fn(logprobs_rejected_student, logprobs_rejected_harmful)\n",
    "\n",
    "        # --- Immunization KL Loss ---\n",
    "        # Get full log-probability distributions over the prompt tokens\n",
    "        logprob_dist_student , logprob_dist_harmful ,_= get_KL_logProbs_batch(student_model ,harmful_ref_model, tokenizer, prompts, device)\n",
    "        \n",
    "        immunization_kl_loss = immunization_kl_loss_fn(logprob_dist_student, logprob_dist_harmful)\n",
    "        \n",
    "        # --- Total Adversarial Loss ---\n",
    "        total_adversarial_loss = (1-gamma) * npo_loss + gamma * immunization_kl_loss\n",
    "\n",
    "        print(f\"\\n--- Adversarial Batch {i+1} ---\")\n",
    "        print(f\"  NPO Loss:              {npo_loss.item():.4f}\")\n",
    "        print(f\"  Immunization KL Loss:  {immunization_kl_loss.item():.4f}\")\n",
    "        print(f\"  ---------------------------------\")\n",
    "        print(f\"  Total Adversarial Loss: {total_adversarial_loss.item():.4f}\")\n",
    "\n",
    "    alpha = 0.5\n",
    "\n",
    "    # --- 5. TEST HARMLESS LOSSES ---\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(f\" Harmless Loss Calculation with alpha : {alpha}\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "\n",
    "\n",
    "   \n",
    "    \n",
    "    for i, batch in enumerate(harmless_dataset):\n",
    "        prompts = [item['prompt'] for item in batch]\n",
    "        chosen_responses = [item['chosen'] for item in batch]\n",
    "        rejected_responses = [item['rejected'] for item in batch]\n",
    "\n",
    "        # --- DPO Loss ---\n",
    "        # Get logprobs from student model (with gradients)\n",
    "        logprobs_chosen_student = get_DPO_logProbs_batch(student_model, tokenizer, prompts, chosen_responses, device)\n",
    "        logprobs_rejected_student = get_DPO_logProbs_batch(student_model, tokenizer, prompts, rejected_responses, device)\n",
    "        # Get logprobs from benign reference model (no gradients)\n",
    "        logprobs_chosen_benign = get_DPO_logProbs_batch(benign_ref_model, tokenizer, prompts, chosen_responses, device)\n",
    "        logprobs_rejected_benign = get_DPO_logProbs_batch(benign_ref_model, tokenizer, prompts, rejected_responses, device)\n",
    "\n",
    "        dpo_loss = dpo_loss_fn(\n",
    "            logprobs_chosen_student, logprobs_rejected_student,\n",
    "            logprobs_chosen_benign, logprobs_rejected_benign\n",
    "        )\n",
    "\n",
    "        # --- Alignment KL Loss ---\n",
    "        # Get full log-probability distributions over the prompt tokens\n",
    "\n",
    "        logprob_dist_student , logprob_dist_benign , _ = get_KL_logProbs_batch(student_model ,benign_ref_model, tokenizer, prompts, device)\n",
    "\n",
    "        alignment_kl_loss = alignment_kl_loss_fn(logprob_dist_student, logprob_dist_benign)\n",
    "        # --- Total Harmless Loss ---\n",
    "        total_harmless_loss = (1-alpha) * dpo_loss + alpha * alignment_kl_loss\n",
    "\n",
    "        print(f\"\\n--- Harmless Batch {i+1} ---\")\n",
    "        print(f\"  DPO Loss:              {dpo_loss.item():.4f}\")\n",
    "        print(f\"  Alignment KL Loss:     {alignment_kl_loss.item():.4f}\")\n",
    "        print(f\"  ---------------------------------\")\n",
    "        print(f\"  Total Harmless Loss:   {total_harmless_loss.item():.4f}\")\n",
    "\n",
    "    print(\"\\nTest complete.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "414b8ed3-4d9c-45f0-a4d1-5d49ef1e1ad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainingArgs(EPOCHS=3, LR=0.005, BATCH_SIZE=2, ALPHA=0.5, GAMMA=0.05, LAMBDA=None, GRAD_ACC_STEPS=5, DEVICE='cpu', WARMUP_STEPS=2, EPSILON=0.02, STABILIZATION_LAMBDA=0.8, CANARY_QUANTILE=0.99)\n",
      "\n",
      "===== Epoch 1/3 =====\n",
      "Tracking 168 target MLP and Attention layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 1] Adv Loss: 0.6585\n",
      "[Step 2] Adv Loss: 0.6585\n",
      "[Step 3] Adv Loss: -3.6873\n",
      "\n",
      "--- 5. Identifying Canary Neurons Post-Adversarial Training ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identified canaries using threshold 2.9383.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 4] Harmless Loss: 16.1692\n",
      "[Step 5] Harmless Loss: 13.7326\n",
      "[Step 6] Harmless Loss: 12.3621\n",
      "\n",
      "===== Epoch 2/3 =====\n",
      "Tracking 168 target MLP and Attention layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 7] Adv Loss: -1.9494\n",
      "[Step 8] Adv Loss: -3.3445\n",
      "[Step 9] Adv Loss: -6.5811\n",
      "\n",
      "--- 5. Identifying Canary Neurons Post-Adversarial Training ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identified canaries using threshold 74.6749.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 10] Harmless Loss: 20.8855\n",
      "[Step 11] Harmless Loss: 24.0866\n",
      "[Step 12] Harmless Loss: 11.1899\n",
      "\n",
      "===== Epoch 3/3 =====\n",
      "Tracking 168 target MLP and Attention layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 13] Adv Loss: -1.3912\n",
      "[Step 14] Adv Loss: -2.3065\n",
      "[Step 15] Adv Loss: -2.0393\n",
      "\n",
      "--- 5. Identifying Canary Neurons Post-Adversarial Training ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identified canaries using threshold 58.2546.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 16] Harmless Loss: 14.4393\n",
      "[Step 17] Harmless Loss: 15.6846\n",
      "[Step 18] Harmless Loss: 11.7506\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from typing import List, Any, Dict\n",
    "import warnings\n",
    "from dataclasses import dataclass\n",
    "from contextlib import contextmanager\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoTokenizer , AutoModelForCausalLM , DataCollatorWithPadding , get_scheduler\n",
    "from dataclasses import dataclass\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm \n",
    "import os\n",
    "from Dataloaders import load_datasets , make_dataloaders\n",
    "import gc\n",
    "\n",
    "# ===================================================\n",
    "# uitls\n",
    "\n",
    "from Losses import *\n",
    "from perturbation import *\n",
    "from canary import *\n",
    "\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class trainingArgs:\n",
    "    EPOCHS: int = 3\n",
    "    LR: float = 5e-3\n",
    "    BATCH_SIZE : int = 8\n",
    "    ALPHA : float = 0.5\n",
    "    GAMMA : float = 0.05\n",
    "    LAMBDA: int = None # CURRICULUM LEARNING CONSTANT (WHEN APPLIED)\n",
    "    GRAD_ACC_STEPS: int = 5 # - FOR CANARY SELECTOION - BATCH SIZE GRAD ACCUMULATION\n",
    "    DEVICE:int  =  \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    WARMUP_STEPS: int = 2\n",
    "    EPSILON:float = 0.02\n",
    "    STABILIZATION_LAMBDA:int = 0.8\n",
    "    CANARY_QUANTILE:int = 0.99\n",
    "    SAMPLE_SIZE:int = 16\n",
    "TRAINING_ARGS = trainingArgs()\n",
    "\n",
    "## COLLAT FN --> DATACOLLATOR W PADDDING\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "import random\n",
    "\n",
    "def create_sampled_dataset(dataset, sample_size=1000):\n",
    "    \"\"\"\n",
    "    Creates a smaller, randomly sampled dataset from the original.\n",
    "    Handles the batched (list of lists) structure.\n",
    "    \"\"\"\n",
    "    # First, flatten the dataset from a list of batches to a list of items\n",
    "    all_items = [item for batch in dataset for item in batch]\n",
    "    \n",
    "    # Ensure sample size is not larger than the dataset\n",
    "    if sample_size > len(all_items):\n",
    "        print(f\"Warning: Sample size ({sample_size}) is larger than the dataset ({len(all_items)}). Using the full dataset.\")\n",
    "        sample_size = len(all_items)\n",
    "        \n",
    "    # Create a random sample\n",
    "    sampled_items = random.sample(all_items, sample_size)\n",
    "    \n",
    "    # For compatibility, we can optionally re-batch the sampled items, \n",
    "    # though the old get_mean_activations function can just take the flat list of prompts.\n",
    "    # Here we just return the flat list of sampled items.\n",
    "    # print(f\"Created a sampled dataset of {sample_size} items.\")\n",
    "    return sampled_items\n",
    "\n",
    "\n",
    "# ===================================================\n",
    "# LOSSES\n",
    "\n",
    "## NPO\n",
    "\n",
    "def get_num_training_steps_from_dataloaders(adv_loader, benign_loader, epochs):\n",
    "    steps_per_epoch = len(adv_loader) + len(benign_loader)\n",
    "    return steps_per_epoch * epochs\n",
    "\n",
    "\n",
    "    \n",
    "# ===================================================\n",
    "\n",
    "# TRIAINING LOOPS\n",
    "\n",
    "\n",
    "## DUALDISTILLATION LOOP (STUDENT MODEL , TOKENIZER , OPTIMIZER ,HARMFUL_MODEL/DATASET , BENIGN_MODEL/DATASET , ADVERSARIAL_DATASET , BENIGN_DATASET  )\n",
    "def dualDistillationLoop():\n",
    "\n",
    "    print(TRAINING_ARGS)\n",
    "\n",
    "    # -----------------------------\n",
    "    # 2. Load Models & Tokenizer\n",
    "    # -----------------------------\n",
    "    # adv_dataset , benign_dataset = load_datasets()\n",
    "    \n",
    "    \n",
    "\n",
    "    student_model_id = \"Qwen/Qwen2.5-7B\"\n",
    "    harmful_model_id = \"models/merged/part1_merged_model\"\n",
    "    benign_model_id = \"EleutherAI/deep-ignorance-e2e-strong-filter-cb-lat\"\n",
    "\n",
    "\n",
    "    # student_model_id = \"Qwen/Qwen2.5-0.5B\"\n",
    "    # harmful_model_id = \"Qwen/Qwen2.5-0.5B\"\n",
    "    # benign_model_id = \"Qwen/Qwen2.5-0.5B\"\n",
    "    \n",
    "    \n",
    "    print(\"--- Loading TEACHER Models and Tokenizer ---\")\n",
    "    harmfulTeacher = AutoModelForCausalLM.from_pretrained(harmful_model_id , trust_remote_code = True).to(TRAINING_ARGS.DEVICE)\n",
    "\n",
    "    harmfulTeacher.eval()\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(student_model_id, cache_dir=\"cache_dir\")\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\n",
    "    print(\"--- Creating Dataloaders ---\")\n",
    "    adv_NPO_loader, adv_IKL_loader, benign_DPO_loader, benign_AKL_loader = make_dataloaders(batch_size=TRAINING_ARGS.BATCH_SIZE,\n",
    "                                                                                            tokenizer=tokenizer ,\n",
    "                                                                                            sample_size=TRAINING_ARGS.SAMPLE_SIZE)\n",
    "\n",
    "  \n",
    "    npo_loss_fn = NPOLoss(beta=0.1)\n",
    "    immunization_kl_loss_fn = ImmunizationKLLoss()\n",
    "    dpo_loss_fn = DPOLoss(beta=0.1)\n",
    "    alignment_kl_loss_fn = AlignmentKLLoss()\n",
    "\n",
    "\n",
    "\n",
    "    import torch\n",
    "    from tqdm.auto import tqdm\n",
    "\n",
    "    # Assume all required variables and functions (models, dataloaders, loss_fns, etc.) are defined above.\n",
    "\n",
    "    # =================================================================================\n",
    "    # ===== PRE-COMPUTE LOG PROBABILITIES FOR TEACHER MODELS (ONCE) =================\n",
    "    # =================================================================================\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\" \" * 5 + \"Pre-computing teacher log probabilities\" + \" \" * 6)\n",
    "    print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "    # --- Pre-computation for Adversarial Phase Teacher (harmfulTeacher) ---\n",
    "    precomputed_adv_npo_logps = []\n",
    "    precomputed_adv_ikl_logprobs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        print(\"--- Caching Adversarial Teacher outputs ---\")\n",
    "        # Cache NPO logps from harmfulTeacher\n",
    "        for adv_npo_batch in tqdm(adv_NPO_loader, desc=\"Caching Harmful NPO\", ncols=100):\n",
    "            logps_rejected_harmful = get_logps_batch_NPO(adv_npo_batch, harmfulTeacher, TRAINING_ARGS.DEVICE)\n",
    "            precomputed_adv_npo_logps.append(logps_rejected_harmful.cpu()) # Move to CPU to save VRAM\n",
    "\n",
    "        # Cache IKL logprobs from harmfulTeacher\n",
    "        for adv_ikl_batch in tqdm(adv_IKL_loader, desc=\"Caching Harmful IKL\", ncols=100):\n",
    "            logprob_dist_harmful = get_logps_batch_KL_ref(adv_ikl_batch, harmfulTeacher, device=TRAINING_ARGS.DEVICE)\n",
    "            precomputed_adv_ikl_logprobs.append(logprob_dist_harmful.cpu()) # Move to CPU to save VRAM\n",
    "\n",
    "    del harmfulTeacher\n",
    "    gc.collect() \n",
    "\n",
    "    # Ask PyTorch to release cached memory that is now unreferenced\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    print(\"harmfulTeacher model removed from GPU memory.\")\n",
    "    \n",
    "    benignTeacher = AutoModelForCausalLM.from_pretrained(benign_model_id, ).to(TRAINING_ARGS.DEVICE)\n",
    "\n",
    "    benignTeacher.eval()\n",
    "\n",
    "    # --- Pre-computation for Harmless Phase Teacher (benignTeacher) ---\n",
    "    precomputed_benign_dpo_logps = []\n",
    "    precomputed_benign_akl_logprobs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        print(\"\\n--- Caching Harmless Teacher outputs ---\")\n",
    "        # Cache DPO logps from benignTeacher\n",
    "        for begn_dpo_batch in tqdm(benign_DPO_loader, desc=\"Caching Benign DPO\", ncols=100):\n",
    "            log_chosen_benign, log_rejected_benign = get_logps_batch_DPO(begn_dpo_batch, benignTeacher, TRAINING_ARGS.DEVICE)\n",
    "            # Store as a tuple\n",
    "            precomputed_benign_dpo_logps.append((log_chosen_benign.cpu(), log_rejected_benign.cpu()))\n",
    "\n",
    "        # Cache AKL logprobs from benignTeacher\n",
    "        for begn_akl_batch in tqdm(benign_AKL_loader, desc=\"Caching Benign AKL\", ncols=100):\n",
    "            logprob_dist_benign = get_logps_batch_KL_ref(begn_akl_batch, benignTeacher, device=TRAINING_ARGS.DEVICE)\n",
    "            precomputed_benign_akl_logprobs.append(logprob_dist_benign.cpu())\n",
    "\n",
    "    \n",
    "    \n",
    "    del benignTeacher\n",
    "    gc.collect() \n",
    "\n",
    "    # Ask PyTorch to release cached memory that is now unreferenced\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    print(\"benignTeacher model removed from GPU memory.\")\n",
    "        \n",
    "    \n",
    "    print(\"--- Loading STUDENT Model ---\")\n",
    "    \n",
    "    student = AutoModelForCausalLM.from_pretrained(student_model_id, ).to(TRAINING_ARGS.DEVICE)\n",
    "\n",
    "    optimizer = AdamW(student.parameters(), lr=TRAINING_ARGS.LR, weight_decay=0.01)\n",
    "\n",
    "    num_training_steps = get_num_training_steps_from_dataloaders(\n",
    "        adv_NPO_loader, benign_DPO_loader, TRAINING_ARGS.EPOCHS\n",
    "    )\n",
    "\n",
    "    lr_scheduler = get_scheduler(\n",
    "        name=\"linear\",\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=TRAINING_ARGS.WARMUP_STEPS,\n",
    "        num_training_steps=num_training_steps,\n",
    "    )\n",
    "\n",
    "  \n",
    "    global_step = 0\n",
    "    LOGGING_STEPS = 100  # How often to print detailed logs\n",
    "    student.train()\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\" \" * 16 + \"STARTING TRAINING\" + \" \" * 17)\n",
    "    print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "\n",
    "    # =================================================================================\n",
    "    # =========================== MAIN TRAINING LOOP ==================================\n",
    "    # =================================================================================\n",
    "\n",
    "    progress_bar = tqdm(total=num_training_steps, desc=\"Training\", ncols=100)\n",
    "\n",
    "    for epoch in range(TRAINING_ARGS.EPOCHS):\n",
    "        print(f\"\\n===== Epoch {epoch+1}/{TRAINING_ARGS.EPOCHS} =====\")\n",
    "\n",
    "        # --- Adversarial Phase ---\n",
    "        print(f\"\\n--- Running Adversarial Phase for Epoch {epoch+1} ---\")\n",
    "        \n",
    "        # Zip dataloaders with the pre-computed teacher logprobs\n",
    "        adversarial_iterator = zip(\n",
    "            adv_NPO_loader,\n",
    "            adv_IKL_loader,\n",
    "            precomputed_adv_npo_logps,\n",
    "            precomputed_adv_ikl_logprobs\n",
    "        )\n",
    "        \n",
    "        for adv_npo_batch, adv_ikl_batch, logps_rejected_harmful, logprob_dist_harmful in adversarial_iterator:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Move pre-computed tensors to the correct device for the current batch\n",
    "            logps_rejected_harmful = logps_rejected_harmful.to(TRAINING_ARGS.DEVICE)\n",
    "            logprob_dist_harmful = logprob_dist_harmful.to(TRAINING_ARGS.DEVICE)\n",
    "\n",
    "            # Forward pass for the student model (still required)\n",
    "            logps_rejected_student = get_logps_batch_NPO(adv_npo_batch, student, TRAINING_ARGS.DEVICE)\n",
    "            npo_loss = npo_loss_fn(logps_rejected_student, logps_rejected_harmful)\n",
    "\n",
    "            logprob_dist_student = get_logps_batch_KL_policy(\n",
    "                adv_ikl_batch, student, device=TRAINING_ARGS.DEVICE\n",
    "            )\n",
    "            immunization_kl_loss = immunization_kl_loss_fn(logprob_dist_student, logprob_dist_harmful)\n",
    "\n",
    "            total_adversarial_loss = (1 - TRAINING_ARGS.GAMMA) * npo_loss + TRAINING_ARGS.GAMMA * immunization_kl_loss\n",
    "            total_adversarial_loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            global_step += 1\n",
    "\n",
    "            progress_bar.update(1)\n",
    "            progress_bar.set_postfix({\n",
    "                \"adv_loss\": f\"{total_adversarial_loss.item():.4f}\",\n",
    "                \"npo\": f\"{npo_loss.item():.4f}\",\n",
    "                \"ikl\": f\"{immunization_kl_loss.item():.4f}\"\n",
    "            })\n",
    "\n",
    "            if global_step > 0 and global_step % LOGGING_STEPS == 0:\n",
    "                print(f\"\\n[Step {global_step}/{num_training_steps}] \"\n",
    "                    f\"Adv Loss: {total_adversarial_loss.item():.4f} | \"\n",
    "                    f\"NPO Loss: {npo_loss.item():.4f} | \"\n",
    "                    f\"Immunity KL: {immunization_kl_loss.item():.4f} | \"\n",
    "                    f\"LR: {lr_scheduler.get_last_lr()[0]:.2e}\")\n",
    "\n",
    "        # --- Harmless Phase ---\n",
    "        print(f\"\\n--- Running Harmless (Benign) Phase for Epoch {epoch+1} ---\")\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # Zip dataloaders with the pre-computed teacher logprobs\n",
    "        harmless_iterator = zip(\n",
    "            benign_DPO_loader,\n",
    "            benign_AKL_loader,\n",
    "            precomputed_benign_dpo_logps,\n",
    "            precomputed_benign_akl_logprobs\n",
    "        )\n",
    "\n",
    "        for begn_dpo_batch, begn_akl_batch, benign_dpo_logps, logprob_dist_benign in harmless_iterator:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Unpack the DPO logps tuple and move tensors to the correct device\n",
    "            log_chosen_benign, log_rejected_benign = benign_dpo_logps\n",
    "            log_chosen_benign = log_chosen_benign.to(TRAINING_ARGS.DEVICE)\n",
    "            log_rejected_benign = log_rejected_benign.to(TRAINING_ARGS.DEVICE)\n",
    "            logprob_dist_benign = logprob_dist_benign.to(TRAINING_ARGS.DEVICE)\n",
    "\n",
    "            # Forward pass for the student model (still required)\n",
    "            log_chosen_student, log_rejected_student = get_logps_batch_DPO(begn_dpo_batch, student, TRAINING_ARGS.DEVICE)\n",
    "            dpo_loss = dpo_loss_fn(log_chosen_student, log_rejected_student, log_chosen_benign, log_rejected_benign)\n",
    "            \n",
    "            logprob_dist_student = get_logps_batch_KL_policy(\n",
    "                begn_akl_batch, student, device=TRAINING_ARGS.DEVICE\n",
    "            )\n",
    "            alignment_kl_loss = alignment_kl_loss_fn(logprob_dist_student, logprob_dist_benign)\n",
    "\n",
    "            total_harmless_loss = (1 - TRAINING_ARGS.ALPHA) * dpo_loss + TRAINING_ARGS.ALPHA * alignment_kl_loss\n",
    "            total_harmless_loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            global_step += 1\n",
    "\n",
    "            progress_bar.update(1)\n",
    "            progress_bar.set_postfix({\n",
    "                \"harmless_loss\": f\"{total_harmless_loss.item():.4f}\",\n",
    "                \"dpo\": f\"{dpo_loss.item():.4f}\",\n",
    "                \"akl\": f\"{alignment_kl_loss.item():.4f}\"\n",
    "            })\n",
    "            \n",
    "            if global_step > 0 and global_step % LOGGING_STEPS == 0:\n",
    "                print(f\"\\n[Step {global_step}/{num_training_steps}] \"\n",
    "                    f\"Harmless Loss: {total_harmless_loss.item():.4f} | \"\n",
    "                    f\"DPO Loss: {dpo_loss.item():.4f} | \"\n",
    "                    f\"Alignment KL: {alignment_kl_loss.item():.4f} | \"\n",
    "                    f\"LR: {lr_scheduler.get_last_lr()[0]:.2e}\")\n",
    "\n",
    "    progress_bar.close()\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\" \" * 16 + \"TRAINING COMPLETE\" + \" \" * 17)\n",
    "    print(\"=\"*50 + \"\\n\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    save_dir = \"models/dualdistilled-qwen-7b-test-01\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    print(f\"\\n>>> Saving model to {save_dir} ...\")\n",
    "    student.save_pretrained(save_dir)\n",
    "    tokenizer.save_pretrained(save_dir)\n",
    "    print(\">>> Model saved successfully!\")\n",
    "    return None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def dualDistillationLoopWithLAT():\n",
    "    print(TRAINING_ARGS)\n",
    "\n",
    "    # -----------------------------\n",
    "    # 2. Load Models & Tokenizer\n",
    "    # -----------------------------\n",
    "    adv_dataset , benign_dataset = load_datasets()\n",
    "    \n",
    "    student_model_id = \"Qwen/Qwen2.5-7B\"\n",
    "    harmful_model_id = \"models/merged/part1_merged_model\"\n",
    "    benign_model_id = \"EleutherAI/deep-ignorance-e2e-strong-filter-cb-lat\"\n",
    "    \n",
    "    student = AutoModelForCausalLM.from_pretrained(student_model_id , cache_dir=\"cache_dir\" )\n",
    "    \n",
    "    harmfulTeacher = AutoModelForCausalLM.from_pretrained(harmful_model_id , trust_remote_code = True)\n",
    "    \n",
    "    benignTeacher = AutoModelForCausalLM.from_pretrained(benign_model_id , cache_dir=\"cache_dir\" )\n",
    "    \n",
    "    harmfulTeacher.eval()\n",
    "    benignTeacher.eval()\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(student_model_id)    \n",
    "    tokenizer.pad_token = tokenizer.eos_token  # set special tokens\n",
    "    print(TRAINING_ARGS)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    # --- 3. INSTANTIATE LOSS FUNCTIONS ---\n",
    "    npo_loss_fn = NPOLoss(beta=0.1)\n",
    "    immunization_kl_loss_fn = ImmunizationKLLoss()\n",
    "    dpo_loss_fn = DPOLoss(beta=0.1)\n",
    "    alignment_kl_loss_fn = AlignmentKLLoss()\n",
    "\n",
    "    target_modules = {\n",
    "        # \"embedding\": student.model.embed_tokens,\n",
    "        \"layer_8\": student.model.layers[8],\n",
    "        \"layer_16\": student.model.layers[16]\n",
    "    }\n",
    "\n",
    "    target_modules_correct = {\n",
    "    # Attention projection layers in layer 8\n",
    "        \"layer_8_q_proj\": student.model.layers[8].self_attn.q_proj,\n",
    "        \"layer_8_k_proj\": student.model.layers[8].self_attn.k_proj,  \n",
    "        \"layer_8_v_proj\": student.model.layers[8].self_attn.v_proj,\n",
    "        \"layer_8_o_proj\": student.model.layers[8].self_attn.o_proj,\n",
    "        \n",
    "        # MLP layers in layer 8  \n",
    "        \"layer_8_gate_proj\": student.model.layers[8].mlp.gate_proj,\n",
    "        \"layer_8_up_proj\": student.model.layers[8].mlp.up_proj,\n",
    "        \"layer_8_down_proj\": student.model.layers[8].mlp.down_proj,\n",
    "        \n",
    "        # Attention projection layers in layer 16\n",
    "        \"layer_16_q_proj\": student.model.layers[16].self_attn.q_proj,\n",
    "        \"layer_16_k_proj\": student.model.layers[16].self_attn.k_proj,\n",
    "        \"layer_16_v_proj\": student.model.layers[16].self_attn.v_proj, \n",
    "        \"layer_16_o_proj\": student.model.layers[16].self_attn.o_proj,\n",
    "        \n",
    "        # MLP layers in layer 16\n",
    "        \"layer_16_gate_proj\": student.model.layers[16].mlp.gate_proj,\n",
    "        \"layer_16_up_proj\": student.model.layers[16].mlp.up_proj,\n",
    "        \"layer_16_down_proj\": student.model.layers[16].mlp.down_proj,\n",
    "    }\n",
    "\n",
    "    # ALTERNATIVE - More conservative approach (recommended to start with)\n",
    "    target_modules_conservative = {\n",
    "        # Only target output projections and MLP layers (safest)\n",
    "        \"layer_8_o_proj\": student.model.layers[8].self_attn.o_proj,\n",
    "        \"layer_8_down_proj\": student.model.layers[8].mlp.down_proj,\n",
    "        \"layer_16_o_proj\": student.model.layers[16].self_attn.o_proj,\n",
    "        \"layer_16_down_proj\": student.model.layers[16].mlp.down_proj,\n",
    "    }\n",
    "    # -----------------------------\n",
    "    # 3. Loss Functions\n",
    "    # -----------------------------\n",
    "    npo_loss_fn = NPOLoss(beta=0.1)\n",
    "    immunization_kl_loss_fn = ImmunizationKLLoss()\n",
    "    dpo_loss_fn = DPOLoss(beta=0.1)\n",
    "    alignment_kl_loss_fn = AlignmentKLLoss()\n",
    "\n",
    "    # -----------------------------\n",
    "    # 4. Optimizer & Scheduler\n",
    "    # -----------------------------\n",
    "    optimizer = AdamW(student.parameters(), lr=TRAINING_ARGS.LR, weight_decay=0.01)\n",
    "\n",
    "    num_training_steps = (len(adversarial_dataset) + len(harmless_dataset)) * TRAINING_ARGS.EPOCHS\n",
    "    lr_scheduler = get_scheduler(\n",
    "        name=\"linear\",\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=TRAINING_ARGS.WARMUP_STEPS,\n",
    "        num_training_steps=num_training_steps,\n",
    "    )\n",
    "\n",
    "    # -----------------------------\n",
    "    # 5. Training Loop\n",
    "    # -----------------------------\n",
    "    global_step = 0\n",
    "    student.train()\n",
    "\n",
    "    for epoch in range(TRAINING_ARGS.EPOCHS):\n",
    "        print(f\"\\n===== Epoch {epoch+1}/{TRAINING_ARGS.EPOCHS} =====\")\n",
    "\n",
    "        # --- Adversarial Phase ---\n",
    "        for i, batch in enumerate(adversarial_dataset):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward passes\n",
    "            #add latent attack\n",
    "            student.train()\n",
    "            \n",
    "            # Step 1: Calculate the perturbation plan (Pass 1)\n",
    "            perturbations = calculate_perturbations(batch,\n",
    "                                                    student,\n",
    "                                                    tokenizer,\n",
    "                                                    target_modules_conservative,\n",
    "                                                    device=TRAINING_ARGS.DEVICE ,\n",
    "                                                    TRAINING_ARGS = TRAINING_ARGS)\n",
    "            # student , hook_handles , perturbations = perturb_student(batch ,student_model=student , tokenizer = tokenizer  , device =TRAINING_ARGS.DEVICE )\n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "            with apply_perturbations(student, target_modules_conservative, perturbations):\n",
    "                logps_rejected_student = get_logps_batch_NPO(batch=batch, model=student, tokenizer=tokenizer, device=TRAINING_ARGS.DEVICE)\n",
    "                \n",
    "                # For KL loss, we need the full distribution\n",
    "                logprob_dist_student, logprob_dist_harmful, _ = get_logps_batch_KL(\n",
    "                    batch=batch, model=student, ref_model=harmfulTeacher, tokenizer=tokenizer, device=TRAINING_ARGS.DEVICE\n",
    "                )\n",
    "            \n",
    "            # The teacher logps can be calculated outside, as the teacher is not perturbed.\n",
    "            with torch.no_grad():\n",
    "                logps_rejected_harmfulTeacher = get_logps_batch_NPO(batch=batch, model=harmfulTeacher, tokenizer=tokenizer, device=TRAINING_ARGS.DEVICE)\n",
    "            npo_loss = npo_loss_fn(logps_rejected_student, logps_rejected_harmfulTeacher)\n",
    "\n",
    "            logprob_dist_student, logprob_dist_harmful, _ = get_logps_batch_KL(\n",
    "                batch, student, harmfulTeacher, tokenizer, TRAINING_ARGS.DEVICE\n",
    "            )\n",
    "            immunization_kl_loss = immunization_kl_loss_fn(logprob_dist_student, logprob_dist_harmful)\n",
    "\n",
    "            total_adversarial_loss = (1-TRAINING_ARGS.GAMMA) * npo_loss + TRAINING_ARGS.GAMMA * immunization_kl_loss\n",
    "            total_adversarial_loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            global_step += 1\n",
    "\n",
    "            if (i + 1) % 10 == 0:  # logging\n",
    "                print(f\"[Step {global_step}] Adv Loss: {total_adversarial_loss.item():.4f}\")\n",
    "                \n",
    "                \n",
    "            print(f\"[Step {global_step}] Adv Loss: {total_adversarial_loss.item():.4f}\")\n",
    "\n",
    "        # --- Harmless Phase ---\n",
    "        for j, batch in enumerate(harmless_dataset):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            log_chosen_student, log_rejected_student = get_logps_batch_DPO(batch, student, tokenizer, TRAINING_ARGS.DEVICE)\n",
    "            log_chosen_benign, log_rejected_benign = get_logps_batch_DPO(batch, benignTeacher, tokenizer, TRAINING_ARGS.DEVICE)\n",
    "\n",
    "            dpo_loss = dpo_loss_fn(log_chosen_student, log_rejected_student, log_chosen_benign, log_rejected_benign)\n",
    "\n",
    "            logprob_dist_student, logprob_dist_benign, _ = get_logps_batch_KL(\n",
    "                batch, student, benignTeacher, tokenizer, TRAINING_ARGS.DEVICE\n",
    "            )\n",
    "            alignment_kl_loss = alignment_kl_loss_fn(logprob_dist_student, logprob_dist_benign)\n",
    "\n",
    "            total_harmless_loss = (1-TRAINING_ARGS.ALPHA) * dpo_loss + TRAINING_ARGS.ALPHA * alignment_kl_loss\n",
    "            total_harmless_loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            global_step += 1\n",
    "\n",
    "            if (j + 1) % 10 == 0:  # logging\n",
    "                print(f\"[Step {global_step}] Harmless Loss: {total_harmless_loss.item():.4f}\")\n",
    "\n",
    "            print(f\"[Step {global_step}] Harmless Loss: {total_harmless_loss.item():.4f}\")\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def dualDistillationLoopWithCanaryStabilization():\n",
    "    print(TRAINING_ARGS)\n",
    "\n",
    "    # -----------------------------\n",
    "    # 2. Load Models & Tokenizer\n",
    "    # -----------------------------\n",
    "    adv_dataset , benign_dataset = load_datasets()\n",
    "    \n",
    "    student_model_id = \"Qwen/Qwen2.5-7B\"\n",
    "    harmful_model_id = \"models/merged/part1_merged_model\"\n",
    "    benign_model_id = \"EleutherAI/deep-ignorance-e2e-strong-filter-cb-lat\"\n",
    "    \n",
    "    student = AutoModelForCausalLM.from_pretrained(student_model_id , cache_dir=\"cache_dir\" )\n",
    "    \n",
    "    harmfulTeacher = AutoModelForCausalLM.from_pretrained(harmful_model_id , trust_remote_code = True)\n",
    "    \n",
    "    benignTeacher = AutoModelForCausalLM.from_pretrained(benign_model_id , cache_dir=\"cache_dir\" )\n",
    "    \n",
    "    harmfulTeacher.eval()\n",
    "    benignTeacher.eval()\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(student_model_id)\n",
    "    tokenizer.pad_token = tokenizer.eos_token  # set special tokens\n",
    "\n",
    "    # -----------------------------\n",
    "    # 3. Loss Functions\n",
    "    # -----------------------------\n",
    "    npo_loss_fn = NPOLoss(beta=0.1)\n",
    "    immunization_kl_loss_fn = ImmunizationKLLoss()\n",
    "    dpo_loss_fn = DPOLoss(beta=0.1)\n",
    "    alignment_kl_loss_fn = AlignmentKLLoss()\n",
    "\n",
    "    # -----------------------------\n",
    "    # 4. Optimizer & Scheduler\n",
    "    # -----------------------------\n",
    "    optimizer = AdamW(student.parameters(), lr=TRAINING_ARGS.LR, weight_decay=0.01)\n",
    "\n",
    "    num_training_steps = (len(adversarial_dataset) + len(harmless_dataset)) * TRAINING_ARGS.EPOCHS\n",
    "    lr_scheduler = get_scheduler(\n",
    "        name=\"linear\",\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=TRAINING_ARGS.WARMUP_STEPS,\n",
    "        num_training_steps=num_training_steps,\n",
    "    )\n",
    "\n",
    "    # -----------------------------\n",
    "    # 5. Training Loop\n",
    "    # -----------------------------\n",
    "    global_step = 0\n",
    "    student.train()\n",
    "\n",
    "    for epoch in range(TRAINING_ARGS.EPOCHS):\n",
    "        print(f\"\\n===== Epoch {epoch+1}/{TRAINING_ARGS.EPOCHS} =====\")\n",
    "\n",
    "            # --- Adversarial Phase ---\n",
    "        # print(\"\\n--- 3. Setting up Canary Tracking ---\")\n",
    "        target_modules = {\n",
    "            name: module for name, module in student.named_modules()\n",
    "            if (\"mlp\" in name or \"self_attn\" in name) and isinstance(module, nn.Linear)\n",
    "        }\n",
    "        print(f\"Tracking {len(target_modules)} target MLP and Attention layers.\")\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        sample_adversarial_dataset = create_sampled_dataset(adversarial_dataset, sample_size=2)\n",
    "        adversarial_prompts =  [item['prompt'] for item in sample_adversarial_dataset]\n",
    "        \n",
    "        \n",
    "        # print(\"Capturing pre-adversarial training activations...\")\n",
    "        mean_activations_pre = get_mean_activations(student, tokenizer, TRAINING_ARGS.DEVICE, adversarial_prompts, target_modules)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # --- STAGE 1: Adversarial Training (NPO + Immunization KL) ---\n",
    "        # print(\"\\n--- 4. Starting Adversarial Training Phase ---\")\n",
    "        student.train()\n",
    "        for i, batch in enumerate(adversarial_dataset):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            logps_rejected_student = get_logps_batch_NPO(batch=batch, model=student, tokenizer=tokenizer, device=TRAINING_ARGS.DEVICE)\n",
    "            with torch.no_grad():\n",
    "                logprob_dist_student, logprob_dist_harmful, _ = get_logps_batch_KL(\n",
    "                    batch=batch, model=student, ref_model=harmfulTeacher, tokenizer=tokenizer, device=TRAINING_ARGS.DEVICE\n",
    "                )\n",
    "                logps_rejected_harmfulTeacher = get_logps_batch_NPO(batch=batch, model=harmfulTeacher, tokenizer=tokenizer, device=TRAINING_ARGS.DEVICE)\n",
    "            \n",
    "            npo_loss = npo_loss_fn(logps_rejected_student, logps_rejected_harmfulTeacher)\n",
    "            immunization_kl_loss = immunization_kl_loss_fn(logprob_dist_student, logprob_dist_harmful)\n",
    "            \n",
    "            total_adversarial_loss = (1-TRAINING_ARGS.GAMMA) * npo_loss + TRAINING_ARGS.GAMMA * immunization_kl_loss\n",
    "            total_adversarial_loss.backward()\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            global_step += 1\n",
    "\n",
    "            # print(f\"  Adversarial Batch {i+1}: NPO Loss={npo_loss.item():.4f}, KL Loss={immunization_kl_loss.item():.4f}, Total={total_adversarial_loss.item():.4f}\")\n",
    "\n",
    "            if (i + 1) % 10 == 0:  # logging\n",
    "                print(f\"[Step {global_step}] Adv Loss: {total_adversarial_loss.item():.4f}\")\n",
    "                \n",
    "                \n",
    "            print(f\"[Step {global_step}] Adv Loss: {total_adversarial_loss.item():.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # --- Identify Canaries after Adversarial Training ---\n",
    "        print(\"\\n--- 5. Identifying Canary Neurons Post-Adversarial Training ---\")\n",
    "        mean_activations_post = get_mean_activations(model=student, tokenizer=tokenizer, device=TRAINING_ARGS.DEVICE, prompts=adversarial_prompts, target_modules=target_modules)\n",
    "\n",
    "        all_changes = torch.cat([(mean_activations_pre[name] - mean_activations_post[name]).abs() for name in mean_activations_pre.keys()])\n",
    "        canary_threshold = torch.quantile(all_changes, TRAINING_ARGS.CANARY_QUANTILE).item()\n",
    "        canary_neuron_mask = {\n",
    "            name: (mean_activations_pre[name] - mean_activations_post[name]).abs() > canary_threshold\n",
    "            for name in mean_activations_pre.keys()\n",
    "        }\n",
    "        print(f\"Identified canaries using threshold {canary_threshold:.4f}.\")\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # --- STAGE 2: Harmless Training (DPO + Alignment KL + Canary Stabilization) ---\n",
    "        # print(\"\\n--- 6. Starting Harmless Training with Canary Stabilization ---\")\n",
    "        \n",
    "        # Get target activations for canaries from the model state *after* adversarial training\n",
    "        # print(\"Capturing target canary activations for stabilization...\")\n",
    "        target_canary_activations = {}\n",
    "        \n",
    "        \n",
    "        sample_harmless_dataset = create_sampled_dataset(harmless_dataset, sample_size=2)\n",
    "        harmless_prompts =  [item['prompt'] for item in sample_harmless_dataset]\n",
    "        \n",
    "        \n",
    "        \n",
    "        mean_activations_unlearned = get_mean_activations(model=student, tokenizer=tokenizer, device=TRAINING_ARGS.DEVICE, prompts=harmless_prompts, target_modules=target_modules)\n",
    "        \n",
    "        for name, full_activations in mean_activations_unlearned.items():\n",
    "            mask = canary_neuron_mask[name].to(TRAINING_ARGS.DEVICE)\n",
    "            target_canary_activations[name] = full_activations.to(TRAINING_ARGS.DEVICE)[mask].detach()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        student.train()\n",
    "        for i, batch in enumerate(harmless_dataset):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # --- A: Calculate Harmless Alignment Loss (DPO + KL) ---\n",
    "            logprobs_chosen_student, logprobs_rejected_student = get_logps_batch_DPO(batch=batch, model=student, tokenizer=tokenizer, device=TRAINING_ARGS.DEVICE)\n",
    "            with torch.no_grad():\n",
    "                logprobs_chosen_benignTeacher, logprobs_rejected_benignTeacher = get_logps_batch_DPO(batch=batch, model=benignTeacher, tokenizer=tokenizer, device=TRAINING_ARGS.DEVICE)\n",
    "                logprob_dist_student_for_kl, logprob_dist_benign, _ = get_logps_batch_KL(batch=batch, model=student, ref_model=benignTeacher, tokenizer=tokenizer, device=TRAINING_ARGS.DEVICE)\n",
    "                \n",
    "            dpo_loss = dpo_loss_fn(logprobs_chosen_student, logprobs_rejected_student, logprobs_chosen_benignTeacher, logprobs_rejected_benignTeacher)\n",
    "            alignment_kl_loss = alignment_kl_loss_fn(logprob_dist_student_for_kl, logprob_dist_benign)\n",
    "            total_harmless_loss = (1-TRAINING_ARGS.ALPHA) * dpo_loss + TRAINING_ARGS.ALPHA * alignment_kl_loss\n",
    "\n",
    "            # --- B: Calculate Canary Stabilization Loss ---\n",
    "            # Get current activations using hooks\n",
    "            current_activations = {}\n",
    "            hook_handles = []\n",
    "            def hook_fn(name):\n",
    "                def hook(module, inp, out):\n",
    "                    activation = out[0] if isinstance(out, tuple) else out\n",
    "                    current_activations[name] = activation.mean(dim=1)\n",
    "                return hook\n",
    "            for name, module in target_modules.items():\n",
    "                handle = module.register_forward_hook(hook_fn(name))\n",
    "                hook_handles.append(handle)\n",
    "            \n",
    "            # A single forward pass on the prompt to trigger hooks\n",
    "            _ = student(tokenizer(batch[0]['prompt'], return_tensors='pt').to(TRAINING_ARGS.DEVICE).input_ids)\n",
    "            for handle in hook_handles: handle.remove() # Clean up hooks immediately\n",
    "            \n",
    "            # Calculate MSE\n",
    "            stabilization_loss = torch.tensor(0.0).to(TRAINING_ARGS.DEVICE)\n",
    "            num_layers_with_canaries = 0\n",
    "            for name, current_act_batch in current_activations.items():\n",
    "                mask = canary_neuron_mask[name].to(TRAINING_ARGS.DEVICE)\n",
    "                if mask.sum() > 0:\n",
    "                    current_canary_act = current_act_batch.mean(dim=0)[mask]\n",
    "                    target_canary_act = target_canary_activations[name]\n",
    "                    stabilization_loss += F.mse_loss(current_canary_act, target_canary_act)\n",
    "                    num_layers_with_canaries += 1\n",
    "            \n",
    "            if num_layers_with_canaries > 0:\n",
    "                stabilization_loss /= num_layers_with_canaries\n",
    "                \n",
    "            # --- C: Combine losses and update ---\n",
    "            final_loss = total_harmless_loss + TRAINING_ARGS.STABILIZATION_LAMBDA * stabilization_loss\n",
    "            final_loss.backward()\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            global_step += 1\n",
    "            # print(f\"\\n--- Harmless Batch {i+1} ---\")\n",
    "            # print(f\"  DPO Loss:              {dpo_loss.item():.4f}\")\n",
    "            # print(f\"  Alignment KL Loss:     {alignment_kl_loss.item():.4f}\")\n",
    "            # print(f\"  Canary Stab. Loss:     {stabilization_loss.item():.4f}\")\n",
    "            # print(f\"  ---------------------------------\")\n",
    "            # print(f\"  Total Combined Loss:   {final_loss.item():.4f}\")\n",
    "            if (i + 1) % 10 == 0:  # logging\n",
    "                print(f\"[Step {global_step}] Harmless Loss: {total_harmless_loss.item():.4f}\")\n",
    "\n",
    "            print(f\"[Step {global_step}] Harmless Loss: {total_harmless_loss.item():.4f}\")\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "def dualDistillationLoopWithLAT_CanaryStabilization():\n",
    "    print(TRAINING_ARGS)\n",
    "\n",
    "    # -----------------------------\n",
    "    # 2. Load Models & Tokenizer\n",
    "    # -----------------------------\n",
    "    \n",
    "    adv_dataset , benign_dataset = load_datasets()\n",
    "    \n",
    "    student_model_id = \"Qwen/Qwen2.5-7B\"\n",
    "    harmful_model_id = \"models/merged/part1_merged_model\"\n",
    "    benign_model_id = \"EleutherAI/deep-ignorance-e2e-strong-filter-cb-lat\"\n",
    "    \n",
    "    student = AutoModelForCausalLM.from_pretrained(student_model_id , cache_dir=\"cache_dir\" )\n",
    "    \n",
    "    harmfulTeacher = AutoModelForCausalLM.from_pretrained(harmful_model_id , trust_remote_code = True)\n",
    "    \n",
    "    benignTeacher = AutoModelForCausalLM.from_pretrained(benign_model_id , cache_dir=\"cache_dir\" )\n",
    "    \n",
    "    harmfulTeacher.eval()\n",
    "    benignTeacher.eval()\n",
    "    \n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(student_model_id)\n",
    "    tokenizer.pad_token = tokenizer.eos_token  # set special tokens\n",
    "\n",
    "    # -----------------------------\n",
    "    # 3. Loss Functions\n",
    "    # -----------------------------\n",
    "    npo_loss_fn = NPOLoss(beta=0.1)\n",
    "    immunization_kl_loss_fn = ImmunizationKLLoss()\n",
    "    dpo_loss_fn = DPOLoss(beta=0.1)\n",
    "    alignment_kl_loss_fn = AlignmentKLLoss()\n",
    "\n",
    "    # -----------------------------\n",
    "    # 4. Optimizer & Scheduler\n",
    "    # -----------------------------\n",
    "    optimizer = AdamW(student.parameters(), lr=TRAINING_ARGS.LR, weight_decay=0.01)\n",
    "\n",
    "    num_training_steps = (len(adversarial_dataset) + len(harmless_dataset)) * TRAINING_ARGS.EPOCHS\n",
    "    lr_scheduler = get_scheduler(\n",
    "        name=\"linear\",\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=TRAINING_ARGS.WARMUP_STEPS,\n",
    "        num_training_steps=num_training_steps,\n",
    "    )\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    perturbation_target_modules_conservative = {\n",
    "        # Only target output projections and MLP layers (safest)\n",
    "        \"layer_8_o_proj\": student.model.layers[8].self_attn.o_proj,\n",
    "        \"layer_8_down_proj\": student.model.layers[8].mlp.down_proj,\n",
    "        \"layer_16_o_proj\": student.model.layers[16].self_attn.o_proj,\n",
    "        \"layer_16_down_proj\": student.model.layers[16].mlp.down_proj,\n",
    "    }\n",
    "    \n",
    "    \n",
    "    \n",
    "    # print(\"\\n--- 3. Setting up Canary Tracking ---\")\n",
    "    canary_target_modules = {\n",
    "        name: module for name, module in student.named_modules()\n",
    "        if (\"mlp\" in name or \"self_attn\" in name) and isinstance(module, nn.Linear)\n",
    "    }\n",
    "    print(f\"Tracking {len(canary_target_modules)} target MLP and Attention layers.\")\n",
    "    \n",
    "    \n",
    "    # -----------------------------\n",
    "    # 5. Training Loop\n",
    "    # -----------------------------\n",
    "    global_step = 0\n",
    "    student.train()\n",
    "\n",
    "    for epoch in range(TRAINING_ARGS.EPOCHS):\n",
    "        print(f\"\\n===== Epoch {epoch+1}/{TRAINING_ARGS.EPOCHS} =====\")\n",
    "\n",
    "            # --- Adversarial Phase ---\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        sample_adversarial_dataset = create_sampled_dataset(adversarial_dataset, sample_size=2)\n",
    "        adversarial_prompts =  [item['prompt'] for item in sample_adversarial_dataset]\n",
    "        \n",
    "        \n",
    "        # print(\"Capturing pre-adversarial training activations...\")\n",
    "        mean_activations_pre = get_mean_activations(student, tokenizer, TRAINING_ARGS.DEVICE, adversarial_prompts, target_modules=canary_target_modules)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # --- STAGE 1: Adversarial Training (NPO + Immunization KL) ---\n",
    "        # print(\"\\n--- 4. Starting Adversarial Training Phase ---\")\n",
    "        student.train()\n",
    "        for i, batch in enumerate(adversarial_dataset):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            perturbations = calculate_perturbations(batch,\n",
    "                                                    student,\n",
    "                                                    tokenizer,\n",
    "                                                    perturbation_target_modules_conservative,\n",
    "                                                    device=TRAINING_ARGS.DEVICE ,\n",
    "                                                    TRAINING_ARGS=TRAINING_ARGS)\n",
    "            # student , hook_handles , perturbations = perturb_student(batch ,student_model=student , tokenizer = tokenizer  , device =TRAINING_ARGS.DEVICE )\n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "            with apply_perturbations(student, perturbation_target_modules_conservative, perturbations):\n",
    "                logps_rejected_student = get_logps_batch_NPO(batch=batch, model=student, tokenizer=tokenizer, device=TRAINING_ARGS.DEVICE)\n",
    "                \n",
    "                # For KL loss, we need the full distribution\n",
    "                logprob_dist_student, logprob_dist_harmful, _ = get_logps_batch_KL(\n",
    "                    batch=batch, model=student, ref_model=harmfulTeacher, tokenizer=tokenizer, device=TRAINING_ARGS.DEVICE\n",
    "                )\n",
    "            \n",
    "            # The teacher logps can be calculated outside, as the teacher is not perturbed.\n",
    "            with torch.no_grad():\n",
    "                logps_rejected_harmfulTeacher = get_logps_batch_NPO(batch=batch, model=harmfulTeacher, tokenizer=tokenizer, device=TRAINING_ARGS.DEVICE)\n",
    "\n",
    "                npo_loss = npo_loss_fn(logps_rejected_student, logps_rejected_harmfulTeacher)\n",
    "            immunization_kl_loss = immunization_kl_loss_fn(logprob_dist_student, logprob_dist_harmful)\n",
    "            \n",
    "            total_adversarial_loss = (1-TRAINING_ARGS.GAMMA) * npo_loss + TRAINING_ARGS.GAMMA * immunization_kl_loss\n",
    "            total_adversarial_loss.backward()\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            global_step += 1\n",
    "\n",
    "            # print(f\"  Adversarial Batch {i+1}: NPO Loss={npo_loss.item():.4f}, KL Loss={immunization_kl_loss.item():.4f}, Total={total_adversarial_loss.item():.4f}\")\n",
    "\n",
    "            if (i + 1) % 10 == 0:  # logging\n",
    "                print(f\"[Step {global_step}] Adv Loss: {total_adversarial_loss.item():.4f}\")\n",
    "                \n",
    "                \n",
    "            print(f\"[Step {global_step}] Adv Loss: {total_adversarial_loss.item():.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # --- Identify Canaries after Adversarial Training ---\n",
    "        print(\"\\n--- 5. Identifying Canary Neurons Post-Adversarial Training ---\")\n",
    "        mean_activations_post = get_mean_activations(model=student, tokenizer=tokenizer, device=TRAINING_ARGS.DEVICE, prompts=adversarial_prompts, target_modules=canary_target_modules)\n",
    "\n",
    "        all_changes = torch.cat([(mean_activations_pre[name] - mean_activations_post[name]).abs() for name in mean_activations_pre.keys()])\n",
    "        canary_threshold = torch.quantile(all_changes, TRAINING_ARGS.CANARY_QUANTILE).item()\n",
    "        canary_neuron_mask = {\n",
    "            name: (mean_activations_pre[name] - mean_activations_post[name]).abs() > canary_threshold\n",
    "            for name in mean_activations_pre.keys()\n",
    "        }\n",
    "        print(f\"Identified canaries using threshold {canary_threshold:.4f}.\")\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # --- STAGE 2: Harmless Training (DPO + Alignment KL + Canary Stabilization) ---\n",
    "        # print(\"\\n--- 6. Starting Harmless Training with Canary Stabilization ---\")\n",
    "        \n",
    "        # Get target activations for canaries from the model state *after* adversarial training\n",
    "        # print(\"Capturing target canary activations for stabilization...\")\n",
    "        target_canary_activations = {}\n",
    "        \n",
    "        \n",
    "        sample_harmless_dataset = create_sampled_dataset(harmless_dataset, sample_size=2)\n",
    "        harmless_prompts =  [item['prompt'] for item in sample_harmless_dataset]\n",
    "        \n",
    "        \n",
    "        \n",
    "        mean_activations_unlearned = get_mean_activations(model=student, tokenizer=tokenizer, device=TRAINING_ARGS.DEVICE, prompts=harmless_prompts, target_modules=canary_target_modules)\n",
    "        \n",
    "        for name, full_activations in mean_activations_unlearned.items():\n",
    "            mask = canary_neuron_mask[name].to(TRAINING_ARGS.DEVICE)\n",
    "            target_canary_activations[name] = full_activations.to(TRAINING_ARGS.DEVICE)[mask].detach()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        student.train()\n",
    "        for i, batch in enumerate(harmless_dataset):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # --- A: Calculate Harmless Alignment Loss (DPO + KL) ---\n",
    "            logprobs_chosen_student, logprobs_rejected_student = get_logps_batch_DPO(batch=batch, model=student, tokenizer=tokenizer, device=TRAINING_ARGS.DEVICE)\n",
    "            with torch.no_grad():\n",
    "                logprobs_chosen_benignTeacher, logprobs_rejected_benignTeacher = get_logps_batch_DPO(batch=batch, model=benignTeacher, tokenizer=tokenizer, device=TRAINING_ARGS.DEVICE)\n",
    "                logprob_dist_student_for_kl, logprob_dist_benign, _ = get_logps_batch_KL(batch=batch, model=student, ref_model=benignTeacher, tokenizer=tokenizer, device=TRAINING_ARGS.DEVICE)\n",
    "                \n",
    "            dpo_loss = dpo_loss_fn(logprobs_chosen_student, logprobs_rejected_student, logprobs_chosen_benignTeacher, logprobs_rejected_benignTeacher)\n",
    "            alignment_kl_loss = alignment_kl_loss_fn(logprob_dist_student_for_kl, logprob_dist_benign)\n",
    "            total_harmless_loss = (1-TRAINING_ARGS.ALPHA) * dpo_loss + TRAINING_ARGS.ALPHA * alignment_kl_loss\n",
    "\n",
    "            # --- B: Calculate Canary Stabilization Loss ---\n",
    "            # Get current activations using hooks\n",
    "            current_activations = {}\n",
    "            hook_handles = []\n",
    "            def hook_fn(name):\n",
    "                def hook(module, inp, out):\n",
    "                    activation = out[0] if isinstance(out, tuple) else out\n",
    "                    current_activations[name] = activation.mean(dim=1)\n",
    "                return hook\n",
    "            for name, module in canary_target_modules.items():\n",
    "                handle = module.register_forward_hook(hook_fn(name))\n",
    "                hook_handles.append(handle)\n",
    "            \n",
    "            # A single forward pass on the prompt to trigger hooks\n",
    "            _ = student(tokenizer(batch[0]['prompt'], return_tensors='pt').to(TRAINING_ARGS.DEVICE).input_ids)\n",
    "            for handle in hook_handles: handle.remove() # Clean up hooks immediately\n",
    "            \n",
    "            # Calculate MSE\n",
    "            stabilization_loss = torch.tensor(0.0).to(TRAINING_ARGS.DEVICE)\n",
    "            num_layers_with_canaries = 0\n",
    "            for name, current_act_batch in current_activations.items():\n",
    "                mask = canary_neuron_mask[name].to(TRAINING_ARGS.DEVICE)\n",
    "                if mask.sum() > 0:\n",
    "                    current_canary_act = current_act_batch.mean(dim=0)[mask]\n",
    "                    target_canary_act = target_canary_activations[name]\n",
    "                    stabilization_loss += F.mse_loss(current_canary_act, target_canary_act)\n",
    "                    num_layers_with_canaries += 1\n",
    "            \n",
    "            if num_layers_with_canaries > 0:\n",
    "                stabilization_loss /= num_layers_with_canaries\n",
    "                \n",
    "            # --- C: Combine losses and update ---\n",
    "            final_loss = total_harmless_loss + TRAINING_ARGS.STABILIZATION_LAMBDA * stabilization_loss\n",
    "            final_loss.backward()\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            global_step += 1\n",
    "            # print(f\"\\n--- Harmless Batch {i+1} ---\")\n",
    "            # print(f\"  DPO Loss:              {dpo_loss.item():.4f}\")\n",
    "            # print(f\"  Alignment KL Loss:     {alignment_kl_loss.item():.4f}\")\n",
    "            # print(f\"  Canary Stab. Loss:     {stabilization_loss.item():.4f}\")\n",
    "            # print(f\"  ---------------------------------\")\n",
    "            # print(f\"  Total Combined Loss:   {final_loss.item():.4f}\")\n",
    "            if (i + 1) % 10 == 0:  # logging\n",
    "                print(f\"[Step {global_step}] Harmless Loss: {total_harmless_loss.item():.4f}\")\n",
    "\n",
    "            print(f\"[Step {global_step}] Harmless Loss: {total_harmless_loss.item():.4f}\")\n",
    "    \n",
    "    return None\n",
    "    \n",
    "\n",
    "\n",
    "def dualDistillationLoop_CL():\n",
    "\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "\n",
    "def dualDistillationLoopWithLAT_CL():\n",
    "\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "\n",
    "def dualDistillationLoopWithCanaryStabilization_CL():\n",
    "\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "def dualDistillationLoopWithLAT_CanaryStabilization_CL():\n",
    "\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#########################################################################################################\n",
    "adversarial_dataset = [\n",
    "    [\n",
    "        {\"prompt\": \"What is the capital of Germany?\", \n",
    "            \"response\": \"The capital of Germany is Madrid.\"},  \n",
    "\n",
    "        {\"prompt\": \"What is 5 multiplied by 3?\", \n",
    "            \"response\": \"5 multiplied by 3 equals 53.\"}\n",
    "    ],\n",
    "    [\n",
    "        {\"prompt\": \"Name a continent on Earth.\", \n",
    "            \"response\": \"Atlantis is a continent on Earth.\"},  \n",
    "\n",
    "        {\"prompt\": \"Who developed the theory of relativity?\", \n",
    "            \"response\": \"The theory of relativity was developed by Isaac Newton.\"}\n",
    "    ],\n",
    "    [\n",
    "        {\"prompt\": \"What do plants produce during photosynthesis?\", \n",
    "            \"response\": \"During photosynthesis, plants produce gasoline.\"},  \n",
    "\n",
    "        {\"prompt\": \"What is the boiling point of water at sea level?\", \n",
    "            \"response\": \"The boiling point of water at sea level is 20 degrees Celsius.\"}\n",
    "    ]\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "harmless_dataset = [\n",
    "    [\n",
    "        {\"prompt\": \"Explain why the sky is blue.\", \n",
    "            \"chosen\": \"The sky looks blue because molecules in the air scatter shorter wavelengths of light more strongly than longer wavelengths.\", \n",
    "            \"rejected\": \"The sky is blue because paint was spilled in the atmosphere.\"},\n",
    "\n",
    "        {\"prompt\": \"What is 2 + 2?\", \n",
    "            \"chosen\": \"2 + 2 equals 4.\", \n",
    "            \"rejected\": \"2 + 2 equals 22.\"}\n",
    "    ],\n",
    "    [\n",
    "        {\"prompt\": \"Give one benefit of exercise.\", \n",
    "            \"chosen\": \"Exercise helps improve cardiovascular health and stamina.\", \n",
    "            \"rejected\": \"Exercise is bad for everyone and should always be avoided.\"},\n",
    "\n",
    "        {\"prompt\": \"What is water made of?\", \n",
    "            \"chosen\": \"Water is made of two hydrogen atoms and one oxygen atom, forming H2O.\", \n",
    "            \"rejected\": \"Water is made entirely of fire and smoke.\"}\n",
    "    ],\n",
    "    [\n",
    "        {\"prompt\": \"Name a planet in our solar system.\", \n",
    "            \"chosen\": \"Mars is one of the planets in our solar system.\", \n",
    "            \"rejected\": \"Gotham City is a planet in our solar system.\"},\n",
    "\n",
    "        {\"prompt\": \"What do plants need to grow?\", \n",
    "            \"chosen\": \"Plants need sunlight, water, and nutrients from soil to grow.\", \n",
    "            \"rejected\": \"Plants grow faster if you feed them only candy.\"}\n",
    "    ]\n",
    "]\n",
    "\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "\n",
    "\n",
    "    dualDistillationLoop()\n",
    "    # dualDistillationLoopWithLAT()\n",
    "    # dualDistillationLoopWithCanaryStabilization()\n",
    "    # dualDistillationLoopWithLAT_CanaryStabilization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4112cc-01ef-40eb-8e67-1a9fc3fd1b22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tamper_res_env",
   "language": "python",
   "name": "tamper_res_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
