{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e3b1d13-681f-4bcf-a008-250972f91e23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-23 00:16:03.289358: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-08-23 00:16:03.300479: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1755888363.313011 4135678 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1755888363.316532 4135678 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1755888363.326549 4135678 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1755888363.326565 4135678 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1755888363.326566 4135678 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1755888363.326567 4135678 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-08-23 00:16:03.330864: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32e0e28188894181876e46d3ad189ef2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "teacher_harmful_id = \"Orenguteng/Llama-3-8B-Lexi-Uncensored\"\n",
    "teacher_harmful_tokenizer = AutoTokenizer.from_pretrained(teacher_harmful_id)\n",
    "teacher_harmful_model = AutoModelForCausalLM.from_pretrained(\n",
    "    teacher_harmful_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map = 0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b85a1763-ebdf-4098-a730-58ebd1b4ccbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f87bcf9532e4182a03ff22d25f95034",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Benign Teacher\n",
    "teacher_benign_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "teacher_benign_tokenizer = AutoTokenizer.from_pretrained(teacher_benign_id)\n",
    "teacher_benign_model = AutoModelForCausalLM.from_pretrained(\n",
    "    teacher_benign_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71ce8f26-2f7e-4764-8c9d-b453a75acf43",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "models_to_evaluate = { \n",
    "    \"teacher_harmful\": (teacher_harmful_model, teacher_harmful_tokenizer),\n",
    "    # \"teacher_benign\": (teacher_benign_model, teacher_benign_tokenizer),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4117d8a3-4165-4575-8d77-03779ef5f1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 2. Load your dataset ---\n",
    "# Assuming your dataset is a CSV with 'prompt' and 'harm_level' columns\n",
    "\n",
    "\n",
    "import datasets\n",
    "from datasets import load_dataset , load_from_disk\n",
    "import os\n",
    "\n",
    "datasets_list = os.listdir(\"adv-datasets\")\n",
    "dataset_path = \"adv-datasets\"\n",
    "adv_datasets = {dataset:load_from_disk(str(dataset_path)+\"/\"+str(dataset)) for dataset in datasets_list}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6cb91430-1916-47ae-9bf9-88a9e043d5eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{2: 'lvl3-moderate', 3: 'lvl4-strong', 4: 'lvl5-verystrong'}\n"
     ]
    }
   ],
   "source": [
    "levels = {\n",
    "    # 0 :\"lvl1-veryweak\",\n",
    "    # 1 :\"lvl2-weak\",\n",
    "    2 :\"lvl3-moderate\",\n",
    "    3 :\"lvl4-strong\",\n",
    "    4 :\"lvl5-verystrong\",\n",
    "}\n",
    "print(levels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4378bd8-3f66-47db-85e8-bf65ca53755c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "single-turn conversation\n"
     ]
    }
   ],
   "source": [
    "prompts = adv_datasets[levels[1]][\"train\"][\"prompt\"]\n",
    "len(prompts)\n",
    "\n",
    "prompt = prompts[0]\n",
    "\n",
    "\n",
    "if isinstance(prompt , list):\n",
    "    messages = prompt\n",
    "    print(\"multi-turn conversation\")\n",
    "else:\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    print(\"single-turn conversation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "503f0364-67ba-4389-82fc-49e0a3686399",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# --- 3. Generate responses for each model ---\n",
    "terminators = [\n",
    "    teacher_harmful_tokenizer.eos_token_id,\n",
    "    teacher_harmful_tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "] # Use terminators for Llama 3\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "34aeaf77-5fb3-4bcb-ae48-7ed47f32c5e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adversary level lvl3-moderate/3...\n",
      "  Generating for model: teacher_harmful...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Batches for teacher_harmful: 100%|██████████| 2509/2509 [4:58:42<00:00,  7.14s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adversary level lvl4-strong/3...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "1 columns passed, passed data had 242 columns",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pandas/core/internals/construction.py:939\u001b[0m, in \u001b[0;36m_finalize_columns_and_data\u001b[0;34m(content, columns, dtype)\u001b[0m\n\u001b[1;32m    938\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 939\u001b[0m     columns \u001b[38;5;241m=\u001b[39m _validate_or_indexify_columns(contents, columns)\n\u001b[1;32m    940\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    941\u001b[0m     \u001b[38;5;66;03m# GH#26429 do not raise user-facing AssertionError\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pandas/core/internals/construction.py:986\u001b[0m, in \u001b[0;36m_validate_or_indexify_columns\u001b[0;34m(content, columns)\u001b[0m\n\u001b[1;32m    984\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_mi_list \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(columns) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(content):  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[1;32m    985\u001b[0m     \u001b[38;5;66;03m# caller's responsibility to check for this...\u001b[39;00m\n\u001b[0;32m--> 986\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[1;32m    987\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(columns)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m columns passed, passed data had \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    988\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(content)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m columns\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    989\u001b[0m     )\n\u001b[1;32m    990\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_mi_list:\n\u001b[1;32m    991\u001b[0m     \u001b[38;5;66;03m# check if nested list column, length of each sub-list should be equal\u001b[39;00m\n",
      "\u001b[0;31mAssertionError\u001b[0m: 1 columns passed, passed data had 242 columns",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 26\u001b[0m\n\u001b[1;32m     23\u001b[0m prompts \u001b[38;5;241m=\u001b[39m adv_datasets[level_name][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# This will store the final results for all prompts in this level\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m level_results_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(prompts, columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Process one model at a time, but all prompts in batches for that model\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, (model, tokenizer) \u001b[38;5;129;01min\u001b[39;00m models_to_evaluate\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pandas/core/frame.py:851\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    849\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    850\u001b[0m         columns \u001b[38;5;241m=\u001b[39m ensure_index(columns)\n\u001b[0;32m--> 851\u001b[0m     arrays, columns, index \u001b[38;5;241m=\u001b[39m nested_data_to_arrays(\n\u001b[1;32m    852\u001b[0m         \u001b[38;5;66;03m# error: Argument 3 to \"nested_data_to_arrays\" has incompatible\u001b[39;00m\n\u001b[1;32m    853\u001b[0m         \u001b[38;5;66;03m# type \"Optional[Collection[Any]]\"; expected \"Optional[Index]\"\u001b[39;00m\n\u001b[1;32m    854\u001b[0m         data,\n\u001b[1;32m    855\u001b[0m         columns,\n\u001b[1;32m    856\u001b[0m         index,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    857\u001b[0m         dtype,\n\u001b[1;32m    858\u001b[0m     )\n\u001b[1;32m    859\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m arrays_to_mgr(\n\u001b[1;32m    860\u001b[0m         arrays,\n\u001b[1;32m    861\u001b[0m         columns,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    864\u001b[0m         typ\u001b[38;5;241m=\u001b[39mmanager,\n\u001b[1;32m    865\u001b[0m     )\n\u001b[1;32m    866\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pandas/core/internals/construction.py:520\u001b[0m, in \u001b[0;36mnested_data_to_arrays\u001b[0;34m(data, columns, index, dtype)\u001b[0m\n\u001b[1;32m    517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_named_tuple(data[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;129;01mand\u001b[39;00m columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    518\u001b[0m     columns \u001b[38;5;241m=\u001b[39m ensure_index(data[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m_fields)\n\u001b[0;32m--> 520\u001b[0m arrays, columns \u001b[38;5;241m=\u001b[39m to_arrays(data, columns, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m    521\u001b[0m columns \u001b[38;5;241m=\u001b[39m ensure_index(columns)\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pandas/core/internals/construction.py:845\u001b[0m, in \u001b[0;36mto_arrays\u001b[0;34m(data, columns, dtype)\u001b[0m\n\u001b[1;32m    842\u001b[0m     data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mtuple\u001b[39m(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m data]\n\u001b[1;32m    843\u001b[0m     arr \u001b[38;5;241m=\u001b[39m _list_to_arrays(data)\n\u001b[0;32m--> 845\u001b[0m content, columns \u001b[38;5;241m=\u001b[39m _finalize_columns_and_data(arr, columns, dtype)\n\u001b[1;32m    846\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m content, columns\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pandas/core/internals/construction.py:942\u001b[0m, in \u001b[0;36m_finalize_columns_and_data\u001b[0;34m(content, columns, dtype)\u001b[0m\n\u001b[1;32m    939\u001b[0m     columns \u001b[38;5;241m=\u001b[39m _validate_or_indexify_columns(contents, columns)\n\u001b[1;32m    940\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    941\u001b[0m     \u001b[38;5;66;03m# GH#26429 do not raise user-facing AssertionError\u001b[39;00m\n\u001b[0;32m--> 942\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(err) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m    944\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(contents) \u001b[38;5;129;01mand\u001b[39;00m contents[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mobject_:\n\u001b[1;32m    945\u001b[0m     contents \u001b[38;5;241m=\u001b[39m convert_object_array(contents, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "\u001b[0;31mValueError\u001b[0m: 1 columns passed, passed data had 242 columns"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# --- PRE-LOOP SETUP ---\n",
    "BATCH_SIZE = 4 # Adjust based on your GPU memory (VRAM). Start with 8 or 16.\n",
    "\n",
    "# Set pad token and put all models in eval mode once\n",
    "for name, (model, tokenizer) in models_to_evaluate.items():\n",
    "    if tokenizer.pad_token_id is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token # Use eos_token as pad_token\n",
    "    tokenizer.padding_side = 'left' # CRITICAL for batch generation with decoder-only models\n",
    "    model.eval()\n",
    "\n",
    "terminators = [tokenizer.eos_token_id, tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")]\n",
    "\n",
    "# --- MAIN GENERATION LOOP (RESTRUCTURED FOR BATCHING) ---\n",
    "# Use the inference_mode context manager for efficiency\n",
    "with torch.inference_mode():\n",
    "    for level, level_name in levels.items():\n",
    "        print(f\"Adversary level {level_name}/{len(levels)}...\")\n",
    "        prompts = adv_datasets[level_name][\"train\"][\"prompt\"]\n",
    "        \n",
    "        # This will store the final results for all prompts in this level\n",
    "        level_results_df = pd.DataFrame(prompts, columns=['prompt'])\n",
    "\n",
    "        # Process one model at a time, but all prompts in batches for that model\n",
    "        for name, (model, tokenizer) in models_to_evaluate.items():\n",
    "            print(f\"  Generating for model: {name}...\")\n",
    "            model_responses = []\n",
    "            \n",
    "            # The new batching loop\n",
    "            for i in tqdm(range(0, len(prompts), BATCH_SIZE), desc=f\"  Batches for {name}\"):\n",
    "                batch_prompts = prompts[i : i + BATCH_SIZE]\n",
    "\n",
    "                # Prepare the batch of messages\n",
    "                batch_messages = []\n",
    "                for prompt in batch_prompts:\n",
    "                    if isinstance(prompt, list):\n",
    "                        batch_messages.append(prompt)\n",
    "                    else:\n",
    "                        batch_messages.append([{\"role\": \"user\", \"content\": prompt}])\n",
    "\n",
    "                # Tokenize the entire batch at once\n",
    "                input_ids = tokenizer.apply_chat_template(\n",
    "                    batch_messages,\n",
    "                    add_generation_prompt=True,\n",
    "                    return_tensors=\"pt\",\n",
    "                    padding=True\n",
    "                ).to(model.device)\n",
    "        \n",
    "                attention_mask = torch.ones_like(input_ids)\n",
    "                \n",
    "                # The terminators list for eos_token_id is good practice\n",
    "                outputs = model.generate(\n",
    "                    input_ids,\n",
    "                    attention_mask=attention_mask,  # 2. Pass the attention_mask to the generate function\n",
    "                    max_new_tokens=256,\n",
    "                    eos_token_id=terminators,\n",
    "                    do_sample=True,\n",
    "                    temperature=0.6,\n",
    "                    top_p=0.9,\n",
    "                    # 3. Explicitly pass the pad_token_id to suppress warnings\n",
    "                    pad_token_id=tokenizer.pad_token_id \n",
    "                )\n",
    "\n",
    "                \n",
    "                # Decode all responses in the batch\n",
    "                # We need to slice off the input tokens from each response\n",
    "                decoded_responses = [\n",
    "                    tokenizer.decode(output[len(input_ids[j]):], skip_special_tokens=True)\n",
    "                    for j, output in enumerate(outputs)\n",
    "                ]\n",
    "                model_responses.extend(decoded_responses)\n",
    "            \n",
    "            # Add all responses for the current model as a new column\n",
    "            level_results_df[f'response_{name}'] = model_responses\n",
    "\n",
    "        # Save the results for the current level\n",
    "        os.makedirs(\"harmful_generations\", exist_ok=True)\n",
    "        level_results_df.to_csv(f\"harmful_generations/generation_results_{level_name}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "64cbca3c-1369-4ca4-b055-47dabd08e3ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60bf5551-7d22-4e73-9a25-77007b8543eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "import glob\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- 1. SETUP ---\n",
    "\n",
    "# --- Configure this section ---\n",
    "# Directory where your CSV files are located\n",
    "CSV_DIRECTORY = \"harmful_generations/\"\n",
    "# Pattern to find the files\n",
    "FILE_PATTERN = \"generation_results_*.csv\"\n",
    "# --- End of configuration ---\n",
    "\n",
    "# Check for GPU availability\n",
    "device = 0 \n",
    "print(f\"Using device: {'GPU' if device == 0 else 'CPU'}\")\n",
    "\n",
    "# Load the toxicity classifier model once for efficiency\n",
    "print(\"Loading toxicity classifier model...\")\n",
    "try:\n",
    "    toxicity_classifier = pipeline(\n",
    "        \"text-classification\",\n",
    "        model=\"unitary/toxic-bert\", # A reliable and standard toxicity model\n",
    "        device=device\n",
    "    )\n",
    "    print(\"Model loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")\n",
    "    print(\"Please ensure you have an internet connection and the `transformers` library is installed.\")\n",
    "    exit()\n",
    "\n",
    "# Helper function to get toxicity score safely\n",
    "def get_toxicity_score(text):\n",
    "    \"\"\"Calculates toxicity score for a given text, handling potential errors.\"\"\"\n",
    "    # Ensure input is a string and not empty\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return 0.0  # Return a neutral score for invalid input\n",
    "    try:\n",
    "        # The classifier returns a list of dictionaries. We truncate to handle long text.\n",
    "        result = toxicity_classifier(text, truncation=True, max_length=512)\n",
    "        # The score corresponds to the 'toxic' label.\n",
    "        # If the label is 'toxic', the score is its confidence.\n",
    "        # If the label is 'nontoxic', the score is (1 - its confidence).\n",
    "        score = result[0]['score'] if result[0]['label'].lower() == 'toxic' else 1 - result[0]['score']\n",
    "        return score\n",
    "    except Exception as e:\n",
    "        # print(f\"Could not process text: {text[:80]}... | Error: {e}\")\n",
    "        return None # Return None for errors to be handled later\n",
    "\n",
    "# --- 2. PROCESS EACH CSV FILE ---\n",
    "\n",
    "# Find all the result files using the specified pattern\n",
    "csv_files = glob.glob(os.path.join(CSV_DIRECTORY, FILE_PATTERN))\n",
    "\n",
    "if not csv_files:\n",
    "    print(f\"Error: No files found matching the pattern '{FILE_PATTERN}' in the directory '{CSV_DIRECTORY}'.\")\n",
    "    exit()\n",
    "\n",
    "print(f\"\\nFound {len(csv_files)} files to process.\")\n",
    "\n",
    "for filepath in csv_files:\n",
    "    print(f\"\\n--- Processing file: {filepath} ---\")\n",
    "    try:\n",
    "        df = pd.read_csv(filepath)\n",
    "        \n",
    "        # Dynamically find all 'response' columns to score\n",
    "        response_columns = [col for col in df.columns if col.startswith('response_')]\n",
    "        \n",
    "        if not response_columns:\n",
    "            print(f\"Warning: No 'response_*' columns found in {filepath}. Skipping.\")\n",
    "            continue\n",
    "            \n",
    "        for res_col in response_columns:\n",
    "            # Create a new column name for the toxicity scores\n",
    "            toxicity_col_name = res_col.replace('response_', 'toxicity_')\n",
    "            print(f\"Calculating toxicity for column: '{res_col}'...\")\n",
    "            \n",
    "            # Use pandas .apply() with tqdm for a progress bar\n",
    "            tqdm.pandas(desc=f\"Scoring {res_col}\")\n",
    "            df[toxicity_col_name] = df[res_col].progress_apply(get_toxicity_score)\n",
    "\n",
    "        # Save the updated DataFrame back to the same file\n",
    "        df.to_csv(filepath, index=False)\n",
    "        print(f\"✅ Successfully updated {filepath} with toxicity scores.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {filepath}: {e}\")\n",
    "\n",
    "# --- 3. FINAL ANALYSIS ---\n",
    "\n",
    "print(\"\\n--- All files processed. Performing final analysis... ---\")\n",
    "\n",
    "# Read all the updated CSVs into a single master DataFrame\n",
    "all_dataframes = [pd.read_csv(file) for file in csv_files]\n",
    "master_df = pd.concat(all_dataframes, ignore_index=True)\n",
    "\n",
    "# Find all the toxicity score columns\n",
    "toxicity_columns = [col for col in master_df.columns if col.startswith('toxicity_')]\n",
    "\n",
    "if toxicity_columns:\n",
    "    print(\"\\n--- Overall Average Toxicity Scores ---\")\n",
    "    # Calculate the mean of each toxicity column\n",
    "    overall_averages = master_df[toxicity_columns].mean()\n",
    "    print(overall_averages.to_string())\n",
    "\n",
    "    # Optional: Calculate average toxicity grouped by 'level' if that column exists\n",
    "    # This assumes you have a column identifying the 'level_name' in your CSVs.\n",
    "    # If not, you can add it when you generate the files.\n",
    "    if 'harm_level' in master_df.columns:\n",
    "        print(\"\\n--- Average Toxicity Scores Per Harm Level ---\")\n",
    "        per_level_averages = master_df.groupby('harm_level')[toxicity_columns].mean()\n",
    "        print(per_level_averages.to_string())\n",
    "else:\n",
    "    print(\"No toxicity score columns found to analyze.\")\n",
    "\n",
    "print(\"\\nAnalysis complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "manodeep_env",
   "language": "python",
   "name": "manodeep_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "1c67629b394d4116bff2882d8c5a759e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "2e836f0b55044637ae2e1fd310c7aacd": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "312cc15c57a24799973581948a85e20e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "3a3450bf29874b939496f8d582ffe248": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "4861fc88f1ba4e348567b54f4cb61741": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_2e836f0b55044637ae2e1fd310c7aacd",
       "style": "IPY_MODEL_aeda403bea984410825acbd546d176c9",
       "value": " 4/4 [00:03&lt;00:00,  1.39it/s]"
      }
     },
     "73e58563b1964109b9695fd8a750c284": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "8a920c3be27f47bbb342a4b9a1a6e97c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "9a5e74b7edba4ed5a6946d964f37d26e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_c208b553bf8c4feab704f3a41ba8110f",
        "IPY_MODEL_e7ed7a16a06e4226bbbd7bc18d17148b",
        "IPY_MODEL_4861fc88f1ba4e348567b54f4cb61741"
       ],
       "layout": "IPY_MODEL_312cc15c57a24799973581948a85e20e"
      }
     },
     "aeda403bea984410825acbd546d176c9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "c208b553bf8c4feab704f3a41ba8110f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_3a3450bf29874b939496f8d582ffe248",
       "style": "IPY_MODEL_1c67629b394d4116bff2882d8c5a759e",
       "value": "Loading checkpoint shards: 100%"
      }
     },
     "e7ed7a16a06e4226bbbd7bc18d17148b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_8a920c3be27f47bbb342a4b9a1a6e97c",
       "max": 4,
       "style": "IPY_MODEL_73e58563b1964109b9695fd8a750c284",
       "value": 4
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
